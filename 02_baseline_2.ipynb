{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-21T08:30:54.314105Z",
     "start_time": "2025-08-21T08:30:50.810619Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from recommenders.datasets.python_splitters import python_chrono_split, python_stratified_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:30:54.398872Z",
     "start_time": "2025-08-21T08:30:54.337744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ],
   "id": "77006ee4de216727",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:00.340076Z",
     "start_time": "2025-08-21T08:30:54.437894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"ratings_electronics.csv\", names=[\"user\", \"item\", \"rating\", \"timestamp\"])\n",
    "df.head()"
   ],
   "id": "61b61e4f02b145d8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             user        item  rating   timestamp\n",
       "0   AKM1MP6P0OYPR  0132793040     5.0  1365811200\n",
       "1  A2CX7LUOHB2NDG  0321732944     5.0  1341100800\n",
       "2  A2NWSAGRHCP8N5  0439886341     1.0  1367193600\n",
       "3  A2WNBOD3WNDNKT  0439886341     3.0  1374451200\n",
       "4  A1GI0U4ZRJA8WN  0439886341     1.0  1334707200"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AKM1MP6P0OYPR</td>\n",
       "      <td>0132793040</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1365811200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2CX7LUOHB2NDG</td>\n",
       "      <td>0321732944</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1341100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A2NWSAGRHCP8N5</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1367193600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A2WNBOD3WNDNKT</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1374451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1GI0U4ZRJA8WN</td>\n",
       "      <td>0439886341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1334707200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:03.936520Z",
     "start_time": "2025-08-21T08:31:00.641536Z"
    }
   },
   "cell_type": "code",
   "source": "df[\"user\"].nunique()",
   "id": "b1d5e838808516ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4201696"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.212722Z",
     "start_time": "2025-08-21T08:31:03.982153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# of interactions, and that every user has at least certain number of interactions\n",
    "def filter_interactions(df, min_user_interactions, min_item_interactions):\n",
    "    while True:\n",
    "        before = len(df)\n",
    "        # Count interactions per user and item\n",
    "        user_counts = df.groupby(\"user\")[\"item\"].count()\n",
    "        item_counts = df.groupby(\"item\")[\"user\"].count()\n",
    "        # Filter users and items based on interaction counts\n",
    "        users_to_keep = set(user_counts[user_counts >= min_user_interactions].index)\n",
    "        items_to_keep = set(item_counts[item_counts >= min_item_interactions].index)\n",
    "        # Filter the DataFrame\n",
    "        df = df[df[\"user\"].isin(users_to_keep) & df[\"item\"].isin(items_to_keep)]\n",
    "        after = len(df)\n",
    "        if after == before:\n",
    "            break\n",
    "    print(f\"Dataframe after interactions filtering: {after}\")\n",
    "    return df\n",
    "\n",
    "df = filter_interactions(df, 10, 10)\n",
    "# df = df.sample(min(300_000, len(df)), random_state=42) # Speed up training by sampling"
   ],
   "id": "50aff7a4f9ea9569",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe after interactions filtering: 347393\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.336152Z",
     "start_time": "2025-08-21T08:31:25.244661Z"
    }
   },
   "cell_type": "code",
   "source": "df.nunique()",
   "id": "f32efb1bcb2cebbd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user         20247\n",
       "item         11589\n",
       "rating           5\n",
       "timestamp     4103\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.635159Z",
     "start_time": "2025-08-21T08:31:25.385996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encode_categorical_columns(df, columns):\n",
    "    for col in columns:\n",
    "        le = LabelEncoder()\n",
    "        df[col + \"_enc\"] = le.fit_transform(df[col])\n",
    "    return df\n",
    "\n",
    "df = encode_categorical_columns(df, [\"user\", \"item\"])"
   ],
   "id": "96ca9b2d1b3b4a72",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.679809Z",
     "start_time": "2025-08-21T08:31:25.676980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def scale_column(df, column):\n",
    "#     scaler = MinMaxScaler()\n",
    "#     df[column + \"_norm\"] = scaler.fit_transform(df[[column]])\n",
    "#     return df\n",
    "#\n",
    "# df = scale_column(df, \"rating\")\n",
    "# df.head()"
   ],
   "id": "74fe0fc199a33f4a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.702501Z",
     "start_time": "2025-08-21T08:31:25.696534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def custom_split(df, require_min_interactions=True, min_interactions=5):\n",
    "#     # Work on a copy to avoid side effects\n",
    "#     tmp = df.copy()\n",
    "#     tmp[\"rank_latest\"] = tmp.groupby(\"user_enc\")[\"timestamp\"] \\\n",
    "#                            .rank(method=\"first\", ascending=False)\n",
    "#\n",
    "#     if require_min_interactions:\n",
    "#         counts = tmp.groupby(\"user_enc\")[\"item_enc\"].transform(\"count\")\n",
    "#         tmp = tmp[counts >= min_interactions]\n",
    "#\n",
    "#     data_test = tmp[tmp[\"rank_latest\"] == 1]\n",
    "#     data_val = tmp[tmp[\"rank_latest\"] == 2]\n",
    "#     data_train = tmp[tmp[\"rank_latest\"] > 2]\n",
    "#\n",
    "#     return (\n",
    "#         data_train.drop(columns=\"rank_latest\"),\n",
    "#         data_val.drop(columns=\"rank_latest\"),\n",
    "#         data_test.drop(columns=\"rank_latest\"),\n",
    "#     )\n",
    "#\n",
    "# data_train, data_val, data_test = custom_split(df)"
   ],
   "id": "f270a72a77d1e11b",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.718030Z",
     "start_time": "2025-08-21T08:31:25.714217Z"
    }
   },
   "cell_type": "code",
   "source": "# len(data_train), len(data_val), len(data_test)",
   "id": "6d975869df9d6073",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.900999Z",
     "start_time": "2025-08-21T08:31:25.725032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_chronological_by_user(df, test_size=0.2):\n",
    "    data_train, data_test = python_chrono_split(\n",
    "        df, ratio=1 - test_size, filter_by=\"user\",\n",
    "        col_user=\"user_enc\", col_item=\"item_enc\", col_timestamp=\"timestamp\"\n",
    "    )\n",
    "    return data_train, data_test\n",
    "\n",
    "def split_random_by_user(df, test_size=0.2):\n",
    "    data_train, data_test = python_stratified_split(\n",
    "        df, ratio=1 - test_size, filter_by=\"user\",\n",
    "        col_user=\"user_enc\", col_item=\"item_enc\"\n",
    "    )\n",
    "    return data_train, data_test\n",
    "\n",
    "data_train, data_test = split_chronological_by_user(df)"
   ],
   "id": "eb61496b9aa200a8",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.914002Z",
     "start_time": "2025-08-21T08:31:25.909002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        return {\n",
    "            \"users\": torch.tensor(row[\"user_enc\"], dtype=torch.long),\n",
    "            \"items\": torch.tensor(row[\"item_enc\"], dtype=torch.long),\n",
    "            \"ratings\": torch.tensor(row[\"rating\"], dtype=torch.float)\n",
    "        }"
   ],
   "id": "4c9a3039d457c52d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:31:25.927128Z",
     "start_time": "2025-08-21T08:31:25.921997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = MFDataset(data_train)\n",
    "test_dataset = MFDataset(data_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2048, shuffle=False)"
   ],
   "id": "86968b7b24196f09",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:32:45.273998Z",
     "start_time": "2025-08-21T08:32:45.264964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MFModel(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_embedding = torch.nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = torch.nn.Embedding(n_items, embedding_dim)\n",
    "        self.user_bias = torch.nn.Embedding(n_users, 1)\n",
    "        self.item_bias = torch.nn.Embedding(n_items, 1)\n",
    "        self.global_bias = torch.nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # init\n",
    "        torch.nn.init.normal_(self.user_embedding.weight, std=0.05)\n",
    "        torch.nn.init.normal_(self.item_embedding.weight, std=0.05)\n",
    "        torch.nn.init.zeros_(self.user_bias.weight)\n",
    "        torch.nn.init.zeros_(self.item_bias.weight)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embeds = self.user_embedding(user_ids)\n",
    "        item_embeds = self.item_embedding(item_ids)\n",
    "        dot_product = (user_embeds * item_embeds).sum(dim=1, keepdim=True)\n",
    "        out = dot_product + self.user_bias(user_ids) + self.item_bias(item_ids) + self.global_bias\n",
    "        return out.squeeze(1)"
   ],
   "id": "312e8e9c1cfd336f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:32:45.779530Z",
     "start_time": "2025-08-21T08:32:45.765077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import torch.nn as nn\n",
    "#\n",
    "# class NeuralMF(nn.Module):\n",
    "#     def __init__(self, n_users, n_items, mf_embedding_dim=32, mlp_embedding_dim=32, mlp_layers=[64, 32, 16]):\n",
    "#         \"\"\"\n",
    "#         Neural Matrix Factorization Model.\n",
    "#\n",
    "#         Args:\n",
    "#             n_users (int): Number of unique users.\n",
    "#             n_items (int): Number of unique items.\n",
    "#             mf_embedding_dim (int): Embedding dimension for the GMF path.\n",
    "#             mlp_embedding_dim (int): Embedding dimension for the MLP path.\n",
    "#             mlp_layers (list of int): A list of layer sizes for the MLP path.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#\n",
    "#         # --- GMF Path ---\n",
    "#         self.mf_user_embedding = nn.Embedding(n_users, mf_embedding_dim)\n",
    "#         self.mf_item_embedding = nn.Embedding(n_items, mf_embedding_dim)\n",
    "#\n",
    "#         # --- MLP Path ---\n",
    "#         self.mlp_user_embedding = nn.Embedding(n_users, mlp_embedding_dim)\n",
    "#         self.mlp_item_embedding = nn.Embedding(n_items, mlp_embedding_dim)\n",
    "#\n",
    "#         # Build the MLP layers dynamically\n",
    "#         mlp_input_dim = mlp_embedding_dim * 2\n",
    "#\n",
    "#         self.mlp = nn.Sequential()\n",
    "#         for i, layer_size in enumerate(mlp_layers):\n",
    "#             self.mlp.add_module(f\"linear_{i}\", nn.Linear(mlp_input_dim, layer_size))\n",
    "#             self.mlp.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "#             self.mlp.add_module(f\"dropout_{i}\", nn.Dropout(0.3)) # Adding dropout for regularization\n",
    "#             mlp_input_dim = layer_size\n",
    "#\n",
    "#         # --- Final Prediction Layer ---\n",
    "#         # The input size is the GMF output dim (mf_embedding_dim) + MLP output dim (last layer size)\n",
    "#         prediction_input_dim = mf_embedding_dim + mlp_layers[-1]\n",
    "#         self.prediction_layer = nn.Linear(prediction_input_dim, 1)\n",
    "#\n",
    "#         # Initialize weights\n",
    "#         self._init_weights()\n",
    "#\n",
    "#     def _init_weights(self):\n",
    "#         # Use Xavier initialization for embeddings and linear layers\n",
    "#         nn.init.xavier_uniform_(self.mf_user_embedding.weight)\n",
    "#         nn.init.xavier_uniform_(self.mf_item_embedding.weight)\n",
    "#         nn.init.xavier_uniform_(self.mlp_user_embedding.weight)\n",
    "#         nn.init.xavier_uniform_(self.mlp_item_embedding.weight)\n",
    "#\n",
    "#         for module in self.mlp.modules():\n",
    "#             if isinstance(module, nn.Linear):\n",
    "#                 nn.init.xavier_uniform_(module.weight)\n",
    "#\n",
    "#         nn.init.xavier_uniform_(self.prediction_layer.weight)\n",
    "#         self.prediction_layer.bias.data.fill_(0.01)\n",
    "#\n",
    "#     def forward(self, users, items):\n",
    "#         # --- GMF Path ---\n",
    "#         mf_user_emb = self.mf_user_embedding(users)\n",
    "#         mf_item_emb = self.mf_item_embedding(items)\n",
    "#         mf_vector = mf_user_emb * mf_item_emb # Element-wise product\n",
    "#\n",
    "#         # --- MLP Path ---\n",
    "#         mlp_user_emb = self.mlp_user_embedding(users)\n",
    "#         mlp_item_emb = self.mlp_item_embedding(items)\n",
    "#         mlp_vector = torch.cat([mlp_user_emb, mlp_item_emb], dim=1)\n",
    "#         mlp_vector = self.mlp(mlp_vector)\n",
    "#\n",
    "#         # --- Concatenate GMF and MLP outputs ---\n",
    "#         combined_vector = torch.cat([mf_vector, mlp_vector], dim=1)\n",
    "#\n",
    "#         # --- Final Prediction ---\n",
    "#         prediction = self.prediction_layer(combined_vector)\n",
    "#\n",
    "#         return prediction.squeeze()"
   ],
   "id": "fc62d0c244f98cc6",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:32:46.372172Z",
     "start_time": "2025-08-21T08:32:46.083450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataiter = next(iter(train_loader))\n",
    "print(dataiter)"
   ],
   "id": "351c4d356d25267c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'users': tensor([ 9871, 17857,  4363,  ..., 19869,  5576,   206]), 'items': tensor([ 7419, 10677,  8970,  ...,  2236,  6270,  2199]), 'ratings': tensor([5., 4., 5.,  ..., 1., 5., 3.])}\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:32:46.772347Z",
     "start_time": "2025-08-21T08:32:46.727103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_users = df[\"user_enc\"].nunique()\n",
    "n_items = df[\"item_enc\"].nunique()\n",
    "\n",
    "user_embed = torch.nn.Embedding(n_users, 32)\n",
    "item_embed = torch.nn.Embedding(n_items, 32)\n",
    "linear = torch.nn.Linear(32, 1)"
   ],
   "id": "14ea1dc436667988",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:32:47.326410Z",
     "start_time": "2025-08-21T08:32:47.312832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ue = user_embed(dataiter[\"users\"])\n",
    "ie = item_embed(dataiter[\"items\"])\n",
    "print(f\"ue {ue.size()}\")\n",
    "print(f\"ue {ie}\")\n",
    "print(f\"ie {ie.size()}\")\n",
    "print(f\"ie {ie}\")"
   ],
   "id": "bb075d7315b53f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ue torch.Size([2048, 32])\n",
      "ue tensor([[ 6.1860e-01, -1.4095e+00, -6.6337e-01,  ..., -5.1911e-01,\n",
      "          3.8000e-01, -6.4145e-01],\n",
      "        [-4.2898e-01, -1.1247e+00,  9.6239e-01,  ...,  7.0314e-01,\n",
      "         -7.9808e-01, -2.0124e+00],\n",
      "        [-2.4749e+00,  6.1989e-01,  2.0604e+00,  ...,  2.7381e+00,\n",
      "          1.2434e+00,  1.7003e+00],\n",
      "        ...,\n",
      "        [ 6.3711e-01,  1.6581e+00, -4.3391e-01,  ..., -2.8884e-01,\n",
      "          1.2085e+00,  6.3904e-01],\n",
      "        [-6.3111e-01,  4.7434e-01, -5.8121e-01,  ..., -3.7808e-01,\n",
      "          1.2493e-01, -3.3517e+00],\n",
      "        [-7.1588e-01, -2.0351e+00,  1.4031e-01,  ...,  6.1738e-01,\n",
      "          3.0129e-01, -8.2433e-04]], grad_fn=<EmbeddingBackward0>)\n",
      "ie torch.Size([2048, 32])\n",
      "ie tensor([[ 6.1860e-01, -1.4095e+00, -6.6337e-01,  ..., -5.1911e-01,\n",
      "          3.8000e-01, -6.4145e-01],\n",
      "        [-4.2898e-01, -1.1247e+00,  9.6239e-01,  ...,  7.0314e-01,\n",
      "         -7.9808e-01, -2.0124e+00],\n",
      "        [-2.4749e+00,  6.1989e-01,  2.0604e+00,  ...,  2.7381e+00,\n",
      "          1.2434e+00,  1.7003e+00],\n",
      "        ...,\n",
      "        [ 6.3711e-01,  1.6581e+00, -4.3391e-01,  ..., -2.8884e-01,\n",
      "          1.2085e+00,  6.3904e-01],\n",
      "        [-6.3111e-01,  4.7434e-01, -5.8121e-01,  ..., -3.7808e-01,\n",
      "          1.2493e-01, -3.3517e+00],\n",
      "        [-7.1588e-01, -2.0351e+00,  1.4031e-01,  ...,  6.1738e-01,\n",
      "          3.0129e-01, -8.2433e-04]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:32:47.977580Z",
     "start_time": "2025-08-21T08:32:47.968427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = (ue * ie).sum(dim=1)\n",
    "print(f\"output {output.size()}\")\n",
    "print(f\"output {output}\")"
   ],
   "id": "982f6e0b4f4b6107",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output torch.Size([2048])\n",
      "output tensor([-0.3017, -1.7871, 11.6894,  ...,  2.8224, 11.4021,  0.9011],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:32:48.191016Z",
     "start_time": "2025-08-21T08:32:48.181154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, optimizer, loss_fn, train_loader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(train_loader, desc=\"Training\", unit=\"batch\", leave=False)\n",
    "    for batch in pbar:\n",
    "        users = batch[\"users\"].to(device)\n",
    "        items = batch[\"items\"].to(device)\n",
    "        ratings = batch[\"ratings\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(users, items)\n",
    "        loss = loss_fn(predictions, ratings)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * users.size(0)\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(model, loss_fn, data_loader, device):\n",
    "    model.eval()\n",
    "    target_rating_list = []\n",
    "    model_output_list = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, desc=\"Evaluating\", unit=\"batch\", leave=False)\n",
    "        for batch in pbar:\n",
    "            users = batch[\"users\"].to(device)\n",
    "            items = batch[\"items\"].to(device)\n",
    "            ratings = batch[\"ratings\"].to(device)\n",
    "\n",
    "            predictions = model(users, items)\n",
    "            predictions = predictions.clamp(min=1.0, max=5.0)\n",
    "            loss = loss_fn(predictions, ratings)\n",
    "            total_loss += loss.item() * users.size(0)\n",
    "\n",
    "            target_rating_list.extend(ratings.cpu().numpy())\n",
    "            model_output_list.extend(predictions.cpu().numpy())\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    rmse = np.sqrt(np.mean((np.array(model_output_list) - np.array(target_rating_list))**2))\n",
    "    avg_loss = total_loss / len(data_loader.dataset)\n",
    "    return avg_loss, rmse"
   ],
   "id": "b4d66116358a2aa2",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T08:35:08.384763Z",
     "start_time": "2025-08-21T08:32:48.392162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_users = df[\"user_enc\"].nunique()\n",
    "n_items = df[\"item_enc\"].nunique()\n",
    "epochs = 10\n",
    "\n",
    "model = MFModel(n_users, n_items, embedding_dim=32).to(DEVICE)\n",
    "# model = NeuralMF(n_users, n_items, mf_embedding_dim=32, mlp_embedding_dim=32, mlp_layers=[64, 32, 16]).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "epoch_train_losses, epoch_val_losses = [], []\n",
    "best_val_rmse = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    # TRAINING\n",
    "    train_loss = train_one_epoch(model, optimizer, loss_fn, train_loader, DEVICE)\n",
    "    epoch_train_losses.append(train_loss)\n",
    "\n",
    "    # VALIDATION (not test set!)\n",
    "    # The lists are now local to the evaluate function, fixing the bug\n",
    "    val_loss, val_rmse = evaluate(model, loss_fn, test_loader, DEVICE)\n",
    "    epoch_val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "    # Model Checkpointing\n",
    "    if val_rmse < best_val_rmse:\n",
    "        best_val_rmse = val_rmse\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"  New best model saved with RMSE: {best_val_rmse:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "print(f\"Best validation RMSE: {best_val_rmse:.4f}\")"
   ],
   "id": "f26c739e4032c5cd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 18.6891 | Val Loss: 12.1352 | Val RMSE: 3.4836\n",
      "  New best model saved with RMSE: 3.4836\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 16.6690 | Val Loss: 12.1352 | Val RMSE: 3.4836\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 14.7952 | Val Loss: 12.1348 | Val RMSE: 3.4835\n",
      "  New best model saved with RMSE: 3.4835\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train Loss: 13.0027 | Val Loss: 11.9615 | Val RMSE: 3.4585\n",
      "  New best model saved with RMSE: 3.4585\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[29]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     20\u001B[39m epoch_train_losses.append(train_loss)\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# VALIDATION (not test set!)\u001B[39;00m\n\u001B[32m     23\u001B[39m \u001B[38;5;66;03m# The lists are now local to the evaluate function, fixing the bug\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m val_loss, val_rmse = \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m epoch_val_losses.append(val_loss)\n\u001B[32m     27\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  Train Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | Val Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m | Val RMSE: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mval_rmse\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 29\u001B[39m, in \u001B[36mevaluate\u001B[39m\u001B[34m(model, loss_fn, data_loader, device)\u001B[39m\n\u001B[32m     27\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m     28\u001B[39m     pbar = tqdm(data_loader, desc=\u001B[33m\"\u001B[39m\u001B[33mEvaluating\u001B[39m\u001B[33m\"\u001B[39m, unit=\u001B[33m\"\u001B[39m\u001B[33mbatch\u001B[39m\u001B[33m\"\u001B[39m, leave=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpbar\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     30\u001B[39m \u001B[43m        \u001B[49m\u001B[43musers\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43musers\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[43m        \u001B[49m\u001B[43mitems\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mitems\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    731\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    732\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    733\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m734\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    735\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    736\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    737\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    739\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    740\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    788\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    789\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m790\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    791\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    792\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = \u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m]\u001B[49m\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m<listcomp>\u001B[39m\u001B[34m(.0)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mMFDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[32m      9\u001B[39m     row = \u001B[38;5;28mself\u001B[39m.dataframe.iloc[idx]\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[32m     11\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33musers\u001B[39m\u001B[33m\"\u001B[39m: torch.tensor(row[\u001B[33m\"\u001B[39m\u001B[33muser_enc\u001B[39m\u001B[33m\"\u001B[39m], dtype=torch.long),\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mitems\u001B[39m\u001B[33m\"\u001B[39m: \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mitem_enc\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlong\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[32m     13\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mratings\u001B[39m\u001B[33m\"\u001B[39m: torch.tensor(row[\u001B[33m\"\u001B[39m\u001B[33mrating\u001B[39m\u001B[33m\"\u001B[39m], dtype=torch.float)\n\u001B[32m     14\u001B[39m     }\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "37845de184b47a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
