{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cross-Domain Recommendation System Development\n",
    "This notebook is an experiment in building a cross-domain recommendation system using the Amazon Reviews dataset. It uses the best model from the single-domain experiments and extends it to handle multiple domains. The dataset is the same as in the single-domain experiments, but now will combine data from two different domains."
   ],
   "id": "f897965258465d1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-27T15:46:53.039408Z",
     "start_time": "2025-08-27T15:46:47.403183Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/Python Projects/recommendation_system\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/data\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/models\"\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = \"E:/Python Scripts/recsys\"\n",
    "# os.environ['HF_DATASETS_CACHE'] = \"E:/Python Scripts/recsys/data\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = \"E:/Python Scripts/recsys/models\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_dataset, Features, Value\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:46:53.080794Z",
     "start_time": "2025-08-27T15:46:53.046628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)"
   ],
   "id": "7c34635975b115fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Single-domain development on best model (SASRec)",
   "id": "f50b5b1c0a2ae4cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:46:53.095855Z",
     "start_time": "2025-08-27T15:46:53.087348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HF_DATASET = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "\n",
    "def load_amazon_reviews(domain, save_dir=\"data\", max_items=None, seed=SEED):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = f\"{save_dir}/amazon_reviews_{domain}.csv\"\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File {filepath} not found. Downloading dataset for domain '{domain}'...\")\n",
    "        ds = load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "            f\"raw_review_{domain}\",\n",
    "            split=\"full\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Keep only needed columns\n",
    "        ds = ds.select_columns([\"user_id\", \"parent_asin\", \"rating\", \"timestamp\"])\n",
    "        ds = ds.rename_columns({\"user_id\": \"user\", \"parent_asin\": \"item\"})\n",
    "        ds = ds.cast(Features({\n",
    "            \"user\": Value(\"string\"),\n",
    "            \"item\": Value(\"string\"),\n",
    "            \"rating\": Value(\"float32\"),\n",
    "            \"timestamp\": Value(\"int64\"),\n",
    "        }))\n",
    "\n",
    "        # Convert to pandas (Arrow zero-copy where possible)\n",
    "        df = ds.to_pandas()\n",
    "        df.insert(3, \"domain\", domain)\n",
    "        df.to_csv(f\"{save_dir}/amazon_reviews_{domain}.csv\", index=False)\n",
    "        print(f\"Saved amazon_reviews_{domain}.csv to {save_dir}/\")\n",
    "\n",
    "    final_df = pd.read_csv(filepath)\n",
    "    # Random subset if max_items is set\n",
    "    if max_items is not None:\n",
    "        k = min(max_items, len(final_df))\n",
    "        final_df = final_df.sample(n=k, random_state=seed).reset_index(drop=True)\n",
    "    print(f\"Loaded {filepath} with {len(final_df)} rows.\")\n",
    "    return final_df\n",
    "\n",
    "def preprocess_dataset(df, min_user_interactions=5, min_item_interactions=5):\n",
    "    # Make it implicit\n",
    "    df[\"label\"] = 1.0\n",
    "    user_counts = df.groupby(\"user\").size()\n",
    "    valid_users = user_counts[user_counts >= min_user_interactions].index\n",
    "    item_counts = df.groupby(\"item\").size()\n",
    "    valid_items = item_counts[item_counts >= min_item_interactions].index\n",
    "    df_filtered = df[df[\"user\"].isin(valid_users) & df[\"item\"].isin(valid_items)]\n",
    "    print(\"After interactions filtering:\", len(df_filtered), \"rows,\", df_filtered[\"user\"].nunique(), \"users,\", df_filtered[\"item\"].nunique(), \"items\")\n",
    "    return df_filtered\n",
    "\n",
    "def label_encoder(df, shift_item_id=False):\n",
    "    user_enc = LabelEncoder()\n",
    "    item_enc = LabelEncoder()\n",
    "    domain_enc = LabelEncoder()\n",
    "    df[\"user_id\"] = user_enc.fit_transform(df[\"user\"])\n",
    "    df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
    "    if shift_item_id:\n",
    "        df[\"item_id\"] = df[\"item_id\"] + 1  # Shift item IDs by 1 to reserve 0 for padding if needed\n",
    "    df[\"domain_id\"] = domain_enc.fit_transform(df[\"domain\"])\n",
    "    return df, user_enc, item_enc, domain_enc"
   ],
   "id": "fea6ebca4ab2462c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset preparation",
   "id": "48521a298928c450"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:10.028584Z",
     "start_time": "2025-08-27T15:46:53.103409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# New input\n",
    "SOURCE_DOMAIN = \"Books\"\n",
    "\n",
    "# Loading data from multiple domains\n",
    "df = load_amazon_reviews(SOURCE_DOMAIN, max_items=10_000_000, seed=SEED)\n",
    "print(f\"Total rows in {SOURCE_DOMAIN}: {len(df)}\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "filtered_df = preprocess_dataset(df, min_user_interactions=20, min_item_interactions=20)\n",
    "df_encoded, user_encoder, item_encoder, domain_encoder = label_encoder(filtered_df, shift_item_id=True)\n",
    "\n",
    "NUM_USERS = df_encoded[\"user_id\"].max() + 1\n",
    "NUM_ITEMS = df_encoded[\"item_id\"].max() + 1\n",
    "NUM_DOMAINS = df_encoded[\"domain_id\"].max() + 1\n",
    "print(f\"Number of users: {NUM_USERS}, Number of items: {NUM_ITEMS}, Number of domains: {NUM_DOMAINS}\")"
   ],
   "id": "872713bce3008a2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/amazon_reviews_Books.csv with 10000000 rows.\n",
      "Total rows in Books: 10000000\n",
      "After interactions filtering: 318168 rows, 24942 users, 53240 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_10812\\3356054.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"user_id\"] = user_enc.fit_transform(df[\"user\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 24942, Number of items: 53241, Number of domains: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_10812\\3356054.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_10812\\3356054.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"item_id\"] = df[\"item_id\"] + 1  # Shift item IDs by 1 to reserve 0 for padding if needed\n",
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_10812\\3356054.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"domain_id\"] = domain_enc.fit_transform(df[\"domain\"])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:11.200661Z",
     "start_time": "2025-08-27T15:48:10.082954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_user_sequences(df):\n",
    "    df_sorted = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "    user_sequences = {}\n",
    "    for uid, group in df_sorted.groupby(\"user_id\"):\n",
    "        items = group[\"item_id\"].tolist()\n",
    "        user_sequences[uid] = items\n",
    "\n",
    "    print(f\"Number of users: {len(user_sequences)}\")\n",
    "    print(f\"Max sequence length: {max(len(seq) for seq in user_sequences.values())}\")\n",
    "    print(f\"Min sequence length: {min(len(seq) for seq in user_sequences.values())}\")\n",
    "\n",
    "    return user_sequences\n",
    "\n",
    "# Create sequences\n",
    "user_sequences = create_user_sequences(df_encoded)\n",
    "pos_items_by_user = {u: set(seq) for u, seq in user_sequences.items()}"
   ],
   "id": "ca58a698977746d5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 24942\n",
      "Max sequence length: 415\n",
      "Min sequence length: 1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:11.356831Z",
     "start_time": "2025-08-27T15:48:11.206748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequences_loo_split(user_sequences):\n",
    "    train_seqs = {}\n",
    "    val_data = {}\n",
    "    test_data = {}\n",
    "\n",
    "    for user, seq in user_sequences.items():\n",
    "        if len(seq) < 3:  # Need at least 3 items for train/val/test\n",
    "            continue\n",
    "\n",
    "        train_seqs[user] = seq[:-2]  # All but last two\n",
    "        val_data[user] = (seq[:-2], seq[-2])  # Train on all but last 2, predict second-to-last\n",
    "        test_data[user] = (seq[:-1], seq[-1])  # Train on all but last, predict last\n",
    "\n",
    "    print(f\"Training sequences: {len(train_seqs)}\")\n",
    "    print(f\"Validation users: {len(val_data)}\")\n",
    "    print(f\"Test users: {len(test_data)}\")\n",
    "\n",
    "    return train_seqs, val_data, test_data\n",
    "\n",
    "train_sequences, val_sequences, test_sequences = sequences_loo_split(user_sequences)\n",
    "print(f\"Sequences - Train: {len(train_sequences)}, Val: {len(val_sequences)}, Test: {len(test_sequences)}\")"
   ],
   "id": "f25ac4008c7f94b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: 22235\n",
      "Validation users: 22235\n",
      "Test users: 22235\n",
      "Sequences - Train: 22235, Val: 22235, Test: 22235\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset and DataLoader\n",
    "SASRec uses sequences of user interactions to predict the next item. So for sequence `[i1, i2, i3, i4]`, the training samples are:\n",
    "- Input: `[i1]` -> Target: `i2`\n",
    "- Input: `[i1, i2]` -> Target: `i3`\n",
    "- Input: `[i1, i2, i3]` -> Target: `i4`"
   ],
   "id": "f04c70046f9e7ad2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:12.054705Z",
     "start_time": "2025-08-27T15:48:11.362851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SASRecDataset(Dataset):\n",
    "    def __init__(self, data, num_items, max_seq_len=50, pos_items_by_user=None, mode=\"train\", neg_samples=1):\n",
    "        self.num_items = num_items\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mode = mode\n",
    "        self.neg_samples = neg_samples\n",
    "        self.all_pos = pos_items_by_user\n",
    "\n",
    "        self.samples = []\n",
    "        if mode == \"train\":\n",
    "            for user, seq in data.items():\n",
    "                for i in range(1, len(seq)):\n",
    "                    self.samples.append({\n",
    "                        \"user\": user,\n",
    "                        \"input_seq\": seq[:i],\n",
    "                        \"target\": seq[i],\n",
    "                        \"full_seq\": seq # For negative sampling\n",
    "                    })\n",
    "        else:\n",
    "            for user, (seq, target) in data.items():\n",
    "                self.samples.append({\n",
    "                    \"user\": user,\n",
    "                    \"input_seq\": seq,\n",
    "                    \"target\": target,\n",
    "                    \"full_seq\": seq + [target]\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        user = sample[\"user\"]\n",
    "        seq = sample[\"input_seq\"]\n",
    "        target = sample[\"target\"]\n",
    "\n",
    "        # Truncate sequence if > max length\n",
    "        if len(seq) > self.max_seq_len:\n",
    "            seq = seq[-self.max_seq_len:]\n",
    "\n",
    "        # Left-pad sequence with zeros\n",
    "        pad_len = self.max_seq_len - len(seq)\n",
    "        padded_seq = [0] * pad_len + seq\n",
    "\n",
    "        # Negative sampling\n",
    "        forbid = self.all_pos[user] if self.all_pos is not None else set(sample[\"full_seq\"])\n",
    "        neg_items = set()\n",
    "\n",
    "        while len(neg_items) < self.neg_samples:\n",
    "            neg = random.randint(1, self.num_items - 1)\n",
    "            if neg not in forbid:\n",
    "                neg_items.add(neg)\n",
    "\n",
    "        return {\n",
    "            \"user\": sample[\"user\"],\n",
    "            \"input_seq\": torch.tensor(padded_seq, dtype=torch.long),\n",
    "            \"target\": torch.tensor(target, dtype=torch.long),\n",
    "            \"neg_items\": torch.tensor(list(neg_items), dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SASRecDataset(train_sequences, NUM_ITEMS, pos_items_by_user=pos_items_by_user, max_seq_len=50, mode=\"train\", neg_samples=1)\n",
    "val_dataset = SASRecDataset(val_sequences, NUM_ITEMS, pos_items_by_user=pos_items_by_user, max_seq_len=50, mode=\"val\", neg_samples=99)\n",
    "test_dataset = SASRecDataset(test_sequences, NUM_ITEMS, pos_items_by_user=pos_items_by_user, max_seq_len=50, mode=\"test\", neg_samples=99)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ],
   "id": "c96da4897f52535c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 247292\n",
      "Validation samples: 22235\n",
      "Test samples: 22235\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:12.070336Z",
     "start_time": "2025-08-27T15:48:12.059766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create data loaders\n",
    "BATCH_SIZE = 4096\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "7716857aa0a90ebd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:12.267226Z",
     "start_time": "2025-08-27T15:48:12.077080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first = next(iter(train_loader))\n",
    "print(\"Sample batch from loader:\")\n",
    "print(\"Input sequence shape:\", first[\"input_seq\"].shape)\n",
    "print(\"Target shape:\", first[\"target\"].shape)\n",
    "print(\"Negative items shape:\", first[\"neg_items\"].shape)\n",
    "\n",
    "print(\"\\nSample input sequence:\")\n",
    "random_index = []\n",
    "for _ in range(5):\n",
    "    random_index.append(random.randint(0, len(train_loader) - 1))\n",
    "\n",
    "for i in random_index:\n",
    "    print(first[\"input_seq\"][i])"
   ],
   "id": "9cf8d7b1e3a87a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch from loader:\n",
      "Input sequence shape: torch.Size([4096, 50])\n",
      "Target shape: torch.Size([4096])\n",
      "Negative items shape: torch.Size([4096, 1])\n",
      "\n",
      "Sample input sequence:\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0, 50395, 50653, 49777, 50954, 50808, 50151])\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,  4552,  4315,\n",
      "        30645, 15719,  7750, 15394,   614, 13048, 14547, 38603,   372,  8043])\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0, 19777, 34902,\n",
      "        19238, 11851, 26181, 11521,  7478, 10300,   724,  7518, 12359,  4403])\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0, 16515, 11141, 12855, 12862])\n",
      "tensor([    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0, 19777, 34902,\n",
      "        19238, 11851, 26181, 11521,  7478, 10300,   724,  7518, 12359,  4403])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create SASRec model",
   "id": "ee51311214a572b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:12.282697Z",
     "start_time": "2025-08-27T15:48:12.271747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Building SASRec model\n",
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(self.dropout(self.relu(self.w1(x))))\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Layer norms\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = PointWiseFeedForward(hidden_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "\n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "\n",
    "        return x\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_items,\n",
    "                 hidden_dim=64,\n",
    "                 max_seq_len=50,\n",
    "                 num_blocks=2,\n",
    "                 num_heads=2,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_items = num_items\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Embedding layers\n",
    "        self.item_embed = nn.Embedding(num_items, hidden_dim, padding_idx=0)\n",
    "        self.positional_embed = nn.Embedding(max_seq_len, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack of SASRec blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            AttentionBlock(hidden_dim, num_heads, dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.item_embed.weight[1:])  # Skip padding idx\n",
    "        nn.init.xavier_normal_(self.positional_embed.weight)\n",
    "\n",
    "    def forward(self, input_seq, candidate_items=None):\n",
    "        batch_size, seq_len = input_seq.shape\n",
    "\n",
    "        # Get item embeddings\n",
    "        item_embeds = self.item_embed(input_seq)  # [B, L, D]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        positions = torch.arange(seq_len, device=input_seq.device).unsqueeze(0)\n",
    "        pos_embeds = self.positional_embed(positions)  # [1, L, D]\n",
    "        x = self.dropout(item_embeds + pos_embeds)\n",
    "\n",
    "        # Create causal attention mask\n",
    "        attn_mask = self._create_causal_mask(seq_len, input_seq.device)\n",
    "        pad_mask = input_seq.eq(0)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask=attn_mask)\n",
    "\n",
    "        # Final layer norm\n",
    "        x = self.ln(x)  # [B, L, D]\n",
    "        x = x.masked_fill(pad_mask.unsqueeze(-1), 0.0)\n",
    "\n",
    "        # If candidate_items provided, score them\n",
    "        if candidate_items is not None:\n",
    "            # Get embeddings for candidate items\n",
    "            cand_emb = self.item_embed(candidate_items) # [B, N, D]\n",
    "\n",
    "            # Use last position's representation for scoring\n",
    "            last_hidden = x[:, -1, :].unsqueeze(1)  # [B, 1, D]\n",
    "\n",
    "            # Compute scores via dot product\n",
    "            scores = torch.matmul(last_hidden, cand_emb.transpose(1, 2)).squeeze(1) # [B, N]\n",
    "            return scores\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _create_causal_mask(self, seq_len, device):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=device, dtype=torch.bool), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        return mask\n",
    "\n",
    "    def predict_next(self, input_seq):\n",
    "        # Get sequence representations\n",
    "        seq_repr = self.forward(input_seq)  # [B, L, D]\n",
    "\n",
    "        # Use last position for prediction\n",
    "        last_hidden = seq_repr[:, -1, :]  # [B, D]\n",
    "\n",
    "        # Score against all item embeddings\n",
    "        all_item_embeds = self.item_embed.weight  # [num_items, D]\n",
    "        scores = torch.matmul(last_hidden, all_item_embeds.T)  # [B, num_items]\n",
    "        return scores"
   ],
   "id": "6af96d98ee3a7051",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and evaluation functions",
   "id": "126fb73d3965ec4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:12.292589Z",
     "start_time": "2025-08-27T15:48:12.287190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_sasrec_epoch(model, train_loader, loss_fn, optimizer, device=\"cpu\"):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_seq = batch[\"input_seq\"].to(device)\n",
    "        pos_items = batch[\"target\"].to(device)\n",
    "        neg_items = batch[\"neg_items\"].to(device)\n",
    "\n",
    "        # Get predictions for last position\n",
    "        seq_output = model(input_seq)  # [B, L, D]\n",
    "        last_hidden = seq_output[:, -1, :]  # [B, D]\n",
    "\n",
    "        # Get embeddings for positive and negative items\n",
    "        pos_embeds = model.item_embed(pos_items)\n",
    "        neg_embeds = model.item_embed(neg_items)\n",
    "\n",
    "        # Compute logits\n",
    "        pos_logits = (last_hidden * pos_embeds).sum(dim=1)\n",
    "        neg_logits = torch.bmm(neg_embeds, last_hidden.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Binary cross-entropy loss with logits\n",
    "        pos_labels = torch.ones_like(pos_logits)\n",
    "        neg_labels = torch.zeros_like(neg_logits)\n",
    "\n",
    "        # Concatenate logits and labels\n",
    "        all_logits = torch.cat([pos_logits.unsqueeze(1), neg_logits], dim=1)\n",
    "        all_labels = torch.cat([pos_labels.unsqueeze(1), neg_labels], dim=1)\n",
    "\n",
    "        loss = loss_fn(all_logits, all_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / n_batches"
   ],
   "id": "89794d62607239d3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:12.305128Z",
     "start_time": "2025-08-27T15:48:12.297115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Validation loss and ranking metrics\n",
    "@torch.no_grad()\n",
    "def evaluate_sasrec(model, eval_loader, loss_fn, k=10, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_hr = 0.0\n",
    "    sum_ndcg = 0.0\n",
    "    sum_prec = 0.0\n",
    "    sum_ap = 0.0\n",
    "\n",
    "    sum_val_loss = 0.0\n",
    "    n_loss_batches = 0\n",
    "\n",
    "    for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "        input_seq = batch[\"input_seq\"].to(device)\n",
    "        target = batch[\"target\"].to(device)\n",
    "        neg_items = batch[\"neg_items\"].to(device)\n",
    "\n",
    "        batch_size = input_seq.size(0)\n",
    "\n",
    "        # Create candidate set: 1 positive + negatives\n",
    "        seq_output = model(input_seq)  # [B, L, D]\n",
    "        last_hidden = seq_output[:, -1, :]  # [B, D]\n",
    "        candidates = torch.cat([\n",
    "            target.unsqueeze(1),  # [B, 1]\n",
    "            neg_items  # [B, neg_samples]\n",
    "        ], dim=1)  # [B, 1 + neg_samples]\n",
    "\n",
    "        # Get embeddings for all candidates\n",
    "        cand_emb = model.item_embed(candidates)  # [B, 1+neg_samples, D]\n",
    "        scores = torch.bmm(cand_emb, last_hidden.unsqueeze(-1)).squeeze(-1)  # [B, 1+neg_samples]\n",
    "\n",
    "        # sanity: positive not in negatives\n",
    "        if torch.any((candidates[:, 1:] == target.unsqueeze(1)).any(dim=1)):\n",
    "            raise RuntimeError(\"Positive item appeared in negatives for some samples.\")\n",
    "\n",
    "        # Loss calculation\n",
    "        pos_scores = scores[:, 0]\n",
    "        neg_scores = scores[:, 1:]\n",
    "        pos_labels = torch.ones_like(scores[:, 0])\n",
    "        neg_labels = torch.zeros_like(scores[:, 1:])\n",
    "        all_scores = torch.cat([pos_scores.unsqueeze(1), neg_scores], dim=1)\n",
    "        all_labels = torch.cat([pos_labels.unsqueeze(1), neg_labels], dim=1)\n",
    "        batch_loss = loss_fn(all_scores.reshape(-1), all_labels.reshape(-1))\n",
    "        sum_val_loss += batch_loss.item()\n",
    "        n_loss_batches += 1\n",
    "\n",
    "        # Calculate metrics\n",
    "        _, full_idx = torch.sort(scores, dim=1, descending=True)\n",
    "        rank = (full_idx == 0).nonzero(as_tuple=True)[1] + 1  # Rank of the positive item (1-based)\n",
    "\n",
    "        hit = (rank <= k).float()\n",
    "        ndcg = torch.where(rank <= k, 1.0 / torch.log2(rank.float() + 1), torch.zeros_like(hit))\n",
    "        precision = hit / float(k)\n",
    "        ap = torch.where(rank <= k, 1.0 / rank.float(), torch.zeros_like(hit))\n",
    "\n",
    "        sum_hr += hit.sum().item()\n",
    "        sum_ndcg += ndcg.sum().item()\n",
    "        sum_prec += precision.sum().item()\n",
    "        sum_ap += ap.sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "    metrics = {\n",
    "        \"HR@K\": sum_hr / total if total else 0.0,\n",
    "        \"NDCG@K\": sum_ndcg / total if total else 0.0,\n",
    "        \"Precision@K\": sum_prec / total if total else 0.0,\n",
    "        \"MAP@K\": sum_ap / total if total else 0.0,\n",
    "        \"Val loss\": sum_val_loss / max(n_loss_batches, 1)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ],
   "id": "206c2ec3b34e4147",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:48:12.317152Z",
     "start_time": "2025-08-27T15:48:12.310133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sasrec_trainer(\n",
    "        model,\n",
    "        train_loader,\n",
    "        eval_loader,\n",
    "        epochs,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        k=10,\n",
    "        device=\"cpu\",\n",
    "        save_dir=\"model\"\n",
    "    ):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.to(device)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    train_losses, val_losses, val_metrics_log = [], [], []\n",
    "    best_ndcg, best_epoch = 0.0, 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Train (batched)\n",
    "        train_loss = train_sasrec_epoch(model, train_loader, loss_fn, optimizer, device=device)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Eval (batched)\n",
    "        m = evaluate_sasrec(model, eval_loader, loss_fn, k=k, device=device)\n",
    "        val_losses.append(m[\"Val loss\"])\n",
    "        val_metrics_log.append({k_: m[k_] for k_ in [\"HR@K\", \"NDCG@K\", \"Precision@K\", \"MAP@K\"]})\n",
    "\n",
    "        # Checkpointing by NDCG\n",
    "        if m[\"NDCG@K\"] > best_ndcg:\n",
    "            best_ndcg = m[\"NDCG@K\"]\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, \"last_model.pth\"))\n",
    "\n",
    "        # TB logs\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Validation\", m[\"Val loss\"], epoch)\n",
    "        writer.add_scalar(f\"Metrics/Val_HR@{k}\", m[\"HR@K\"], epoch)\n",
    "        writer.add_scalar(f\"Metrics/Val_NDCG@{k}\", m[\"NDCG@K\"], epoch)\n",
    "        writer.add_scalar(f\"Metrics/Val_Precision@{k}\", m[\"Precision@K\"], epoch)\n",
    "        writer.add_scalar(f\"Metrics/Val_MAP@{k}\", m[\"MAP@K\"], epoch)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}  \"\n",
    "            f\"Train loss {train_loss:.4f}  \"\n",
    "            f\"Val loss {m['Val loss']:.4f}  \"\n",
    "            f\"HR@{k} {m['HR@K']:.4f}  \"\n",
    "            f\"NDCG@{k} {m['NDCG@K']:.4f}  \"\n",
    "            f\"Precision@{k} {m['Precision@K']:.4f}  \"\n",
    "            f\"MAP@{k} {m['MAP@K']:.4f}  \"\n",
    "            f\"{'(new best)' if m['NDCG@K'] == best_ndcg and best_epoch==epoch+1 else ''}  \"\n",
    "            f\"Time {time.time()-t0:.2f}s\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nTraining Complete.\")\n",
    "    print(f\"Best epoch: {best_epoch} with NDCG@{k}: {best_ndcg:.4f}\\n\")\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    writer.close()\n",
    "    return train_losses, val_losses, val_metrics_log, best_ndcg"
   ],
   "id": "2cb1cd31cab2153b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training the model",
   "id": "2199dc5aecc7393a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:53:55.095149Z",
     "start_time": "2025-08-27T15:48:12.322666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters from the original paper, except higher hidden_dim\n",
    "sasrec = SASRec(\n",
    "    num_items=NUM_ITEMS,\n",
    "    hidden_dim=64,\n",
    "    max_seq_len=50,\n",
    "    num_blocks=2,\n",
    "    num_heads=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "loss_fn_sasrec = nn.BCEWithLogitsLoss()\n",
    "optimizer_sasrec = torch.optim.Adam(sasrec.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "train_losses_sasrec, val_losses_sasrec, val_metrics_sasrec, best_ndcg_sasrec = sasrec_trainer(\n",
    "    model=sasrec,\n",
    "    train_loader=train_loader,\n",
    "    eval_loader=val_loader,\n",
    "    loss_fn=loss_fn_sasrec,\n",
    "    optimizer=optimizer_sasrec,\n",
    "    epochs=20,\n",
    "    k=10,\n",
    "    device=DEVICE,\n",
    "    save_dir=\"model_sasrec\"\n",
    ")"
   ],
   "id": "52c46a03093c0707",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:15<00:00,  3.95it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20  Train loss 0.6532  Val loss 0.6672  HR@10 0.2449  NDCG@10 0.1296  Precision@10 0.0245  MAP@10 0.0947  (new best)  Time 18.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:15<00:00,  4.05it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20  Train loss 0.5963  Val loss 0.6330  HR@10 0.2468  NDCG@10 0.1299  Precision@10 0.0247  MAP@10 0.0946  (new best)  Time 17.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.18it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20  Train loss 0.5821  Val loss 0.6206  HR@10 0.2503  NDCG@10 0.1310  Precision@10 0.0250  MAP@10 0.0951  (new best)  Time 16.94s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.26it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20  Train loss 0.5661  Val loss 0.6202  HR@10 0.2708  NDCG@10 0.1425  Precision@10 0.0271  MAP@10 0.1039  (new best)  Time 16.99s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.31it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20  Train loss 0.5463  Val loss 0.6167  HR@10 0.2918  NDCG@10 0.1547  Precision@10 0.0292  MAP@10 0.1134  (new best)  Time 16.51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.15it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20  Train loss 0.5250  Val loss 0.6141  HR@10 0.3095  NDCG@10 0.1655  Precision@10 0.0310  MAP@10 0.1220  (new best)  Time 17.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.29it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20  Train loss 0.5040  Val loss 0.6228  HR@10 0.3295  NDCG@10 0.1761  Precision@10 0.0329  MAP@10 0.1297  (new best)  Time 16.92s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.33it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20  Train loss 0.4799  Val loss 0.6014  HR@10 0.3414  NDCG@10 0.1845  Precision@10 0.0341  MAP@10 0.1370  (new best)  Time 16.50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.22it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20  Train loss 0.4539  Val loss 0.5934  HR@10 0.3647  NDCG@10 0.1968  Precision@10 0.0365  MAP@10 0.1460  (new best)  Time 16.90s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.22it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20  Train loss 0.4268  Val loss 0.5658  HR@10 0.3789  NDCG@10 0.2055  Precision@10 0.0379  MAP@10 0.1530  (new best)  Time 16.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.32it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20  Train loss 0.4001  Val loss 0.5340  HR@10 0.3943  NDCG@10 0.2143  Precision@10 0.0394  MAP@10 0.1598  (new best)  Time 16.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.31it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20  Train loss 0.3736  Val loss 0.4992  HR@10 0.4063  NDCG@10 0.2248  Precision@10 0.0406  MAP@10 0.1697  (new best)  Time 16.51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.25it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20  Train loss 0.3509  Val loss 0.4764  HR@10 0.4192  NDCG@10 0.2345  Precision@10 0.0419  MAP@10 0.1784  (new best)  Time 16.74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.24it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20  Train loss 0.3264  Val loss 0.4420  HR@10 0.4334  NDCG@10 0.2434  Precision@10 0.0433  MAP@10 0.1854  (new best)  Time 17.13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.31it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20  Train loss 0.3046  Val loss 0.4137  HR@10 0.4448  NDCG@10 0.2524  Precision@10 0.0445  MAP@10 0.1938  (new best)  Time 16.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.10it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20  Train loss 0.2843  Val loss 0.3919  HR@10 0.4545  NDCG@10 0.2622  Precision@10 0.0454  MAP@10 0.2035  (new best)  Time 17.40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:15<00:00,  3.93it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20  Train loss 0.2652  Val loss 0.3607  HR@10 0.4621  NDCG@10 0.2701  Precision@10 0.0462  MAP@10 0.2113  (new best)  Time 17.95s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.22it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20  Train loss 0.2468  Val loss 0.3422  HR@10 0.4682  NDCG@10 0.2771  Precision@10 0.0468  MAP@10 0.2185  (new best)  Time 16.87s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.16it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20  Train loss 0.2307  Val loss 0.3182  HR@10 0.4748  NDCG@10 0.2830  Precision@10 0.0475  MAP@10 0.2240  (new best)  Time 17.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 61/61 [00:14<00:00,  4.21it/s]\n",
      "Evaluating: 100%|██████████| 6/6 [00:02<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20  Train loss 0.2152  Val loss 0.2969  HR@10 0.4830  NDCG@10 0.2906  Precision@10 0.0483  MAP@10 0.2315  (new best)  Time 16.89s\n",
      "\n",
      "Training Complete.\n",
      "Best epoch: 20 with NDCG@10: 0.2906\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross-domain development",
   "id": "b431468b22f94a82"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:55:33.373242Z",
     "start_time": "2025-08-27T15:55:33.308514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load trained model on source domain\n",
    "def load_best_weights(model, ckpt_path=\"model/best_model.pth\", device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "best_model = load_best_weights(sasrec, ckpt_path=\"model_sasrec/best_model.pth\", device=DEVICE)"
   ],
   "id": "56a95e546e8757d1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Align users across domains",
   "id": "f20f9dbcb0fe38c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T16:52:46.278336Z",
     "start_time": "2025-08-27T16:52:46.272802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def compute_user_reprs_from_sequences(model_src, train_seqs_src, user_encoder_src, max_seq_len=50, device=DEVICE):\n",
    "    model_src.eval().to(device)\n",
    "    user_vecs = {}\n",
    "\n",
    "    for user_id, seq in train_seqs_src.items():\n",
    "        if len(seq) < 1:\n",
    "            continue\n",
    "\n",
    "        # Pad-left to max_seq_len\n",
    "        seq = seq[-max_seq_len:]\n",
    "        pad_len = max_seq_len - len(seq)\n",
    "        input_seq = torch.tensor([([0] * pad_len + seq)], dtype=torch.long, device=device)\n",
    "        hidden = model_src(input_seq)\n",
    "        last_hidden = hidden[0, -1, :].squeeze(0)\n",
    "        raw_user = user_encoder_src.inverse_transform([user_id])[0]\n",
    "        user_vecs[raw_user] = last_hidden.detach().cpu().numpy()\n",
    "\n",
    "    print(f\"\\nComputed user representations for {len(user_vecs)} users.\")\n",
    "    return user_vecs"
   ],
   "id": "58042328ee681734",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T16:57:55.825220Z",
     "start_time": "2025-08-27T16:52:46.479167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cross-domain evaluation on target domain\n",
    "SOURCE_DOMAIN = \"Books\"\n",
    "TARGET_DOMAIN = \"Movies_and_TV\"\n",
    "ALL_DOMAIN = [SOURCE_DOMAIN, TARGET_DOMAIN]\n",
    "\n",
    "# Load data from target domain\n",
    "df_target = load_amazon_reviews(TARGET_DOMAIN, max_items=10_000_000, seed=SEED)\n",
    "filtered_df_target = preprocess_dataset(df_target, min_user_interactions=20, min_item_interactions=20)\n",
    "df_target_encoded, user_encoder_tgt, item_encoder_tgt, domain_encoder_tgt = label_encoder(filtered_df_target, shift_item_id=True)\n",
    "\n",
    "NUM_USERS_TGT = df_target_encoded[\"user_id\"].max() + 1\n",
    "NUM_ITEMS_TGT = df_target_encoded[\"item_id\"].max() + 1\n",
    "\n",
    "# Rebuild sequences for target domain and split\n",
    "user_sequences_tgt = create_user_sequences(df_target_encoded)\n",
    "pos_items_by_user_tgt = {u: set(seq) for u, seq in user_sequences_tgt.items()}\n",
    "train_sequences_tgt, val_sequences_tgt, test_sequences_tgt = sequences_loo_split(user_sequences_tgt)\n",
    "\n",
    "# Build source user vectors from the trained source model\n",
    "user_vecs_src = compute_user_reprs_from_sequences(\n",
    "    model_src=sasrec,\n",
    "    train_seqs_src=train_sequences,\n",
    "    user_encoder_src=user_encoder,\n",
    "    max_seq_len=50,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Create an aligned matrix of source vectors in target's user_id space\n",
    "embed_dim = 64\n",
    "transfer_src_mat = np.zeros((NUM_USERS_TGT, embed_dim), dtype=np.float32)\n",
    "for raw_user, vec in user_vecs_src.items():\n",
    "    if raw_user in user_encoder_tgt.classes_:\n",
    "        uid_target = user_encoder_tgt.transform([raw_user])[0]\n",
    "        transfer_src_mat[uid_target] = vec # give source user vector to target user_id (shared users)\n",
    "\n",
    "transfer_src_mat = torch.tensor(transfer_src_mat)  # [U_T, D]\n",
    "print(\"\\n\")\n",
    "print(transfer_src_mat)"
   ],
   "id": "85d2e182c914668a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/amazon_reviews_Movies_and_TV.csv with 10000000 rows.\n",
      "After interactions filtering: 838690 rows, 29948 users, 73928 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_10812\\3356054.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"user_id\"] = user_enc.fit_transform(df[\"user\"])\n",
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_10812\\3356054.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_10812\\3356054.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"item_id\"] = df[\"item_id\"] + 1  # Shift item IDs by 1 to reserve 0 for padding if needed\n",
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_10812\\3356054.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"domain_id\"] = domain_enc.fit_transform(df[\"domain\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 29948\n",
      "Max sequence length: 1123\n",
      "Min sequence length: 1\n",
      "Training sequences: 29819\n",
      "Validation users: 29819\n",
      "Test users: 29819\n",
      "\n",
      "Computed user representations for 22235 users.\n",
      "\n",
      "\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 2.7717, -7.0042, -1.5810,  ..., -5.7022, -1.2490, -2.1327]])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset and DataLoader for cross-domain",
   "id": "8fe47cc91330d75d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T16:57:55.858987Z",
     "start_time": "2025-08-27T16:57:55.850469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Additional dataset changes for cross-domain\n",
    "class SASRecDatasetCD(SASRecDataset):\n",
    "    def __init__(self, data, num_items, transfer_src_mat, max_seq_len=50, mode=\"train\", neg_samples=1):\n",
    "        super().__init__(data, num_items, max_seq_len=max_seq_len, mode=mode, neg_samples=neg_samples)\n",
    "        self.transfer_src_mat = transfer_src_mat\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out = super().__getitem__(idx)\n",
    "        user_id = out[\"user\"]\n",
    "        out[\"transfer_src\"] = self.transfer_src_mat[user_id].float()\n",
    "        return out"
   ],
   "id": "fb352b01f59749f8",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T17:12:33.453232Z",
     "start_time": "2025-08-27T17:12:30.431712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Target datasets & loaders (reuse your batch size)\n",
    "train_dataset_tgt = SASRecDatasetCD(train_sequences_tgt, NUM_ITEMS_TGT, transfer_src_mat, max_seq_len=50, mode=\"train\", neg_samples=1)\n",
    "val_dataset_tgt = SASRecDatasetCD(val_sequences_tgt, NUM_ITEMS_TGT, transfer_src_mat, max_seq_len=50, mode=\"val\", neg_samples=99)\n",
    "test_dataset_tgt = SASRecDatasetCD(test_sequences_tgt, NUM_ITEMS_TGT, transfer_src_mat, max_seq_len=50, mode=\"test\", neg_samples=99)\n",
    "\n",
    "train_loader_tgt = DataLoader(train_dataset_tgt, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_tgt   = DataLoader(val_dataset_tgt,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader_tgt  = DataLoader(test_dataset_tgt,  batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "id": "c95053be10a6e3af",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cross-domain SASRec model\n",
    "This technique is inspired by the paper [Personalized Transfer of User Preferences for Cross-domain Recommendation (2021)](https://arxiv.org/abs/2110.11154)."
   ],
   "id": "822d0dc78c291e13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T17:12:33.467428Z",
     "start_time": "2025-08-27T17:12:33.459742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SASRecCD(nn.Module):\n",
    "    def __init__(self, base_sasrec, hidden_dim=64, bridge_hidden=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.base = base_sasrec\n",
    "        self.bridge = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, bridge_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(bridge_hidden, bridge_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.gate = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "    def forward(self, input_seq, transfer_src=None, candidate_items=None):\n",
    "        seq_output = self.base(input_seq)\n",
    "        last_hidden = seq_output[:, -1, :]\n",
    "\n",
    "        if transfer_src is not None:\n",
    "            bridge_out = self.bridge(transfer_src)\n",
    "            combined = torch.cat([last_hidden, bridge_out], dim=-1)\n",
    "            gate = torch.sigmoid(self.gate(combined))\n",
    "            fused = gate * last_hidden + (1.0 - gate) * bridge_out\n",
    "        else:\n",
    "            fused = last_hidden\n",
    "\n",
    "        if candidate_items is not None:\n",
    "            cand_emb = self.base.item_embed(candidate_items)\n",
    "            scores = torch.bmm(cand_emb, fused.unsqueeze(-1)).squeeze(-1)\n",
    "            return scores\n",
    "\n",
    "        return fused"
   ],
   "id": "4df69b2ba0f29f83",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training and evaluation functions for cross-domain",
   "id": "e272ab095ba4ce41"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T17:12:34.987539Z",
     "start_time": "2025-08-27T17:12:34.981548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch_transfer(model, loader, loss_fn, optimizer, device=\"cpu\"):\n",
    "    model.train()\n",
    "    total, n = 0.0, 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        inp = batch[\"input_seq\"].to(device)\n",
    "        pos = batch[\"target\"].to(device)\n",
    "        neg = batch[\"neg_items\"].to(device)\n",
    "        transfer = batch[\"transfer_src\"].to(device)\n",
    "\n",
    "        # fused representation\n",
    "        fused = model(inp, transfer_src=transfer)\n",
    "        pos_emb = model.base.item_embed(pos)\n",
    "        neg_emb = model.base.item_embed(neg)\n",
    "\n",
    "        pos_logits = (fused * pos_emb).sum(dim=1)\n",
    "        neg_logits = torch.bmm(neg_emb, fused.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        all_logits = torch.cat([pos_logits.unsqueeze(1), neg_logits], 1)\n",
    "        all_labels = torch.cat([torch.ones_like(pos_logits).unsqueeze(1),\n",
    "                                torch.zeros_like(neg_logits)], 1)\n",
    "\n",
    "        loss = loss_fn(all_logits.reshape(-1), all_labels.reshape(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item(); n += 1\n",
    "    return total / n"
   ],
   "id": "9b5d3f950196aeee",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T17:12:35.401278Z",
     "start_time": "2025-08-27T17:12:35.394618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_transfer(model, loader, loss_fn, k=10, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total = hits = ndcgs = precs = mrrs = 0.0\n",
    "    loss_sum, nb = 0.0, 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "        inp = batch[\"input_seq\"].to(device)\n",
    "        tgt = batch[\"target\"].to(device)\n",
    "        neg = batch[\"neg_items\"].to(device)\n",
    "        transfer = batch[\"transfer_src\"].to(device)\n",
    "\n",
    "        fused = model(inp, transfer_src=transfer)\n",
    "        cand = torch.cat([tgt.unsqueeze(1), neg], dim=1)\n",
    "        cand_emb = model.base.item_embed(cand)\n",
    "        scores = torch.bmm(cand_emb, fused.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # loss (same as train for parity)\n",
    "        labels = torch.cat([torch.ones_like(scores[:, :1]),\n",
    "                            torch.zeros_like(scores[:, 1:])], dim=1)\n",
    "        batch_loss = loss_fn(scores.reshape(-1), labels.reshape(-1))\n",
    "        loss_sum += batch_loss.item(); nb += 1\n",
    "\n",
    "        # ranks & metrics\n",
    "        _, idx = torch.sort(scores, dim=1, descending=True)\n",
    "        rank = (idx == 0).nonzero(as_tuple=True)[1] + 1  # 1-based\n",
    "        hit = (rank <= k).float()\n",
    "        ndcg = torch.where(rank <= k, 1.0 / torch.log2(rank.float() + 1), torch.zeros_like(hit))\n",
    "        precision = hit / float(k)\n",
    "        mrr = 1.0 / rank.float()\n",
    "\n",
    "        B = inp.size(0)\n",
    "        hits += hit.sum().item()\n",
    "        ndcgs += ndcg.sum().item()\n",
    "        precs += precision.sum().item()\n",
    "        mrrs += mrr.sum().item()\n",
    "        total += B\n",
    "\n",
    "    return {\n",
    "        \"HR@K\": hits / total,\n",
    "        \"NDCG@K\": ndcgs / total,\n",
    "        \"Precision@K\": precs / total,\n",
    "        \"MRR\": mrrs / total,\n",
    "        \"Val loss\": loss_sum / max(nb, 1)\n",
    "    }"
   ],
   "id": "c04555ca03e8ea0",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T17:17:42.712859Z",
     "start_time": "2025-08-27T17:17:42.706752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Trainer (target domain)\n",
    "def train_target_with_transfer(model, train_loader, val_loader, epochs, lr=1e-3, wd=1e-6, k=10, device=\"cpu\"):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    best_ndcg, best_epoch = 0.0, 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train = train_epoch_transfer(model, train_loader, loss_fn, opt, device=device)\n",
    "        eval = evaluate_transfer(model, val_loader, loss_fn, k=k, device=device)\n",
    "\n",
    "        if eval[\"NDCG@K\"] > best_ndcg:\n",
    "            best_ndcg, best_epoch = eval[\"NDCG@K\"], epoch+1\n",
    "            torch.save(model.state_dict(), \"xfer_best.pth\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}  \"\n",
    "              f\"Train {train:.4f}  \"\n",
    "              f\"Val {eval['Val loss']:.4f}  \"\n",
    "              f\"HR@{k} {eval['HR@K']:.4f}  \"\n",
    "              f\"NDCG@{k} {eval['NDCG@K']:.4f}  \"\n",
    "              f\"Prec@{k} {eval['Precision@K']:.4f}  \"\n",
    "              f\"MRR {eval['MRR']:.4f}  \"\n",
    "              f\"{'(new best)' if eval['NDCG@K']==best_ndcg and best_epoch==epoch+1 else ''}\")\n",
    "\n",
    "    print(f\"\\nBest epoch {best_epoch} NDCG@{k}={best_ndcg:.4f}\")\n",
    "    return best_ndcg"
   ],
   "id": "c51f2eaaf80216e0",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T17:17:43.486684Z",
     "start_time": "2025-08-27T17:17:43.281168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sasrec_base = load_best_weights(sasrec, ckpt_path=\"model_sasrec/best_model.pth\", device=DEVICE)\n",
    "xfer_model = SASRecCD(sasrec_base, hidden_dim=64, bridge_hidden=128, dropout=0.2)\n",
    "\n",
    "best_ndcg_tgt = train_target_with_transfer(\n",
    "    SASRecCD, train_loader_tgt, val_loader_tgt, epochs=20, lr=1e-3, wd=1e-6, k=10, device=DEVICE\n",
    ")"
   ],
   "id": "ec9016ad1cf5c06",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_apply'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m sasrec_base = load_best_weights(sasrec, ckpt_path=\u001B[33m\"\u001B[39m\u001B[33mmodel_sasrec/best_model.pth\u001B[39m\u001B[33m\"\u001B[39m, device=DEVICE)\n\u001B[32m      2\u001B[39m xfer_model = SASRecCD(sasrec_base, hidden_dim=\u001B[32m64\u001B[39m, bridge_hidden=\u001B[32m128\u001B[39m, dropout=\u001B[32m0.2\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m best_ndcg_tgt = \u001B[43mtrain_target_with_transfer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mSASRecCD\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader_tgt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader_tgt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwd\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mDEVICE\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[42]\u001B[39m\u001B[32m, line 3\u001B[39m, in \u001B[36mtrain_target_with_transfer\u001B[39m\u001B[34m(model, train_loader, val_loader, epochs, lr, wd, k, device)\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain_target_with_transfer\u001B[39m(model, train_loader, val_loader, epochs, lr=\u001B[32m1e-3\u001B[39m, wd=\u001B[32m1e-6\u001B[39m, k=\u001B[32m10\u001B[39m, device=\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m     \u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      4\u001B[39m     opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n\u001B[32m      5\u001B[39m     loss_fn = nn.BCEWithLogitsLoss()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python Projects\\recommendation_system\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1369\u001B[39m, in \u001B[36mModule.to\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1366\u001B[39m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1367\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1369\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_apply\u001B[49m(convert)\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute '_apply'"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c3dbde3410be4fdb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
