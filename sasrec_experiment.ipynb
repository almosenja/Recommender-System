{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-27T09:04:26.408377Z",
     "start_time": "2025-08-27T09:04:03.372707Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = \"D:/Python Projects/recommendation_system\"\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/data\"\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/models\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"E:/Python Scripts/recsys\"\n",
    "os.environ['HF_DATASETS_CACHE'] = \"E:/Python Scripts/recsys/data\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"E:/Python Scripts/recsys/models\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_dataset, Features, Value\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:04:26.530233Z",
     "start_time": "2025-08-27T09:04:26.420401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)"
   ],
   "id": "7c34635975b115fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset preparation",
   "id": "a9855a3fa27e8d92"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:09:55.845800Z",
     "start_time": "2025-08-27T09:09:55.831373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HF_DATASET = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "\n",
    "def load_amazon_reviews(domain:str,\n",
    "                        save_dir:str = \"data\",\n",
    "                        max_items:int | None = None,\n",
    "                        seed:int = SEED) -> pd.DataFrame:\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = f\"{save_dir}/amazon_reviews_{domain}.csv\"\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File {filepath} not found. Downloading dataset for domain '{domain}'...\")\n",
    "        ds = load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "            f\"raw_review_{domain}\",\n",
    "            split=\"full\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Keep only needed columns\n",
    "        ds = ds.select_columns([\"user_id\", \"parent_asin\", \"rating\", \"timestamp\"])\n",
    "        ds = ds.rename_columns({\"user_id\": \"user\", \"parent_asin\": \"item\"})\n",
    "        ds = ds.cast(Features({\n",
    "            \"user\": Value(\"string\"),\n",
    "            \"item\": Value(\"string\"),\n",
    "            \"rating\": Value(\"float32\"),\n",
    "            \"timestamp\": Value(\"int64\"),\n",
    "        }))\n",
    "\n",
    "        # Convert to pandas (Arrow zero-copy where possible)\n",
    "        df = ds.to_pandas()\n",
    "        df.insert(3, \"domain\", domain)\n",
    "        df.to_csv(f\"{save_dir}/amazon_reviews_{domain}.csv\", index=False)\n",
    "        print(f\"Saved amazon_reviews_{domain}.csv to {save_dir}/\")\n",
    "\n",
    "    final_df = pd.read_csv(filepath)\n",
    "    # Random subset if max_items is set\n",
    "    if max_items is not None:\n",
    "        k = min(max_items, len(final_df))\n",
    "        final_df = final_df.sample(n=k, random_state=seed).reset_index(drop=True)\n",
    "    print(f\"Loaded {filepath} with {len(final_df)} rows.\")\n",
    "    return final_df\n",
    "\n",
    "def preprocess_dataset(df, min_user_interactions=5, min_item_interactions=5):\n",
    "    # Make it implicit\n",
    "    df[\"label\"] = 1.0\n",
    "    user_counts = df.groupby(\"user\").size()\n",
    "    valid_users = user_counts[user_counts >= min_user_interactions].index\n",
    "    item_counts = df.groupby(\"item\").size()\n",
    "    valid_items = item_counts[item_counts >= min_item_interactions].index\n",
    "    df_filtered = df[df[\"user\"].isin(valid_users) & df[\"item\"].isin(valid_items)]\n",
    "    print(\"After interactions filtering:\", len(df), \"rows,\", df[\"user\"].nunique(), \"users,\", df[\"item\"].nunique(), \"items\")\n",
    "    return df_filtered\n",
    "\n",
    "def label_encoder(df, shift_item_id=False):\n",
    "    user_enc = LabelEncoder()\n",
    "    item_enc = LabelEncoder()\n",
    "    domain_enc = LabelEncoder()\n",
    "    df[\"user_id\"] = user_enc.fit_transform(df[\"user\"])\n",
    "    df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
    "    if shift_item_id:\n",
    "        df[\"item_id\"] = df[\"item_id\"] + 1  # Shift item IDs by 1 to reserve 0 for padding if needed\n",
    "    df[\"domain_id\"] = domain_enc.fit_transform(df[\"domain\"])\n",
    "    return df, user_enc, item_enc, domain_enc"
   ],
   "id": "fea6ebca4ab2462c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:11:01.821043Z",
     "start_time": "2025-08-27T09:09:56.641162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# New input\n",
    "SOURCE_DOMAIN = \"Books\"\n",
    "\n",
    "# Loading data from multiple domains\n",
    "df = load_amazon_reviews(SOURCE_DOMAIN, max_items=3_000_000, seed=SEED)\n",
    "print(f\"Total rows in {SOURCE_DOMAIN}: {len(df)}\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "filtered_df = preprocess_dataset(df, min_user_interactions=20, min_item_interactions=20)\n",
    "df_encoded, user_encoder, item_encoder, domain_encoder = label_encoder(df, shift_item_id=True)\n",
    "\n",
    "NUM_USERS = df_encoded[\"user_id\"].max() + 1\n",
    "NUM_ITEMS = df_encoded[\"item_id\"].max() + 1\n",
    "NUM_DOMAINS = df_encoded[\"domain_id\"].max() + 1\n",
    "print(f\"Number of users: {NUM_USERS}, Number of items: {NUM_ITEMS}, Number of domains: {NUM_DOMAINS}\")"
   ],
   "id": "872713bce3008a2d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/amazon_reviews_Books.csv with 3000000 rows.\n",
      "Total rows in Books: 3000000\n",
      "After interactions filtering: 3000000 rows, 2089252 users, 1227560 items\n",
      "Number of users: 2089252, Number of items: 1227561, Number of domains: 1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:12:57.272716Z",
     "start_time": "2025-08-27T09:11:50.426325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_user_sequences(df):\n",
    "    df_sorted = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "    user_sequences = {}\n",
    "    for uid, group in df_sorted.groupby(\"user_id\"):\n",
    "        items = group[\"item_id\"].tolist()\n",
    "        user_sequences[uid] = items\n",
    "    return user_sequences\n",
    "\n",
    "# Create sequences\n",
    "user_sequences = create_user_sequences(df_encoded)"
   ],
   "id": "ca58a698977746d5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:14:00.666818Z",
     "start_time": "2025-08-27T09:14:00.538691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequences_loo_split(user_sequences):\n",
    "    \"\"\"\n",
    "    For each user: last item → test, second-to-last → validation, rest → training\n",
    "    \"\"\"\n",
    "    train_seqs = {}\n",
    "    val_data = {}\n",
    "    test_data = {}\n",
    "\n",
    "    for user, seq in user_sequences.items():\n",
    "        if len(seq) < 2:  # Need at least 3 items for train/val/test\n",
    "            continue\n",
    "\n",
    "        train_seqs[user] = seq[:-2]  # All but last two\n",
    "        val_data[user] = (seq[:-2], seq[-2])  # Train on all but last 2, predict second-to-last\n",
    "        test_data[user] = (seq[:-1], seq[-1])  # Train on all but last, predict last\n",
    "\n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"  Training sequences: {len(train_seqs)}\")\n",
    "    print(f\"  Validation users: {len(val_data)}\")\n",
    "    print(f\"  Test users: {len(test_data)}\")\n",
    "\n",
    "    return train_seqs, val_data, test_data\n",
    "\n",
    "train_sequences, val_sequences, test_sequences = sequences_loo_split(df_encoded)\n",
    "print(f\"Sequences - Train: {len(train_sequences)}, Val: {len(val_sequences)}, Test: {len(test_sequences)}\")"
   ],
   "id": "f25ac4008c7f94b",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-2",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:413\u001B[39m, in \u001B[36mRangeIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m    412\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_range\u001B[49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    414\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "\u001B[31mValueError\u001B[39m: -2 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     20\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m  Test users: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_data)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     22\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m train_seqs, val_data, test_data\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m train_sequences, val_sequences, test_sequences = \u001B[43msequences_loo_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_encoded\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mSequences - Train: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(train_sequences)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Val: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(val_sequences)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, Test: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(test_sequences)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 14\u001B[39m, in \u001B[36msequences_loo_split\u001B[39m\u001B[34m(user_sequences)\u001B[39m\n\u001B[32m     11\u001B[39m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[32m     13\u001B[39m     train_seqs[user] = seq[:-\u001B[32m2\u001B[39m]  \u001B[38;5;66;03m# All but last two\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m     val_data[user] = (seq[:-\u001B[32m2\u001B[39m], \u001B[43mseq\u001B[49m\u001B[43m[\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m]\u001B[49m)  \u001B[38;5;66;03m# Train on all but last 2, predict second-to-last\u001B[39;00m\n\u001B[32m     15\u001B[39m     test_data[user] = (seq[:-\u001B[32m1\u001B[39m], seq[-\u001B[32m1\u001B[39m])  \u001B[38;5;66;03m# Train on all but last, predict last\u001B[39;00m\n\u001B[32m     17\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33mData split:\u001B[39m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1130\u001B[39m, in \u001B[36mSeries.__getitem__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1127\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[key]\n\u001B[32m   1129\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m key_is_scalar:\n\u001B[32m-> \u001B[39m\u001B[32m1130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1132\u001B[39m \u001B[38;5;66;03m# Convert generator to list before going through hashable part\u001B[39;00m\n\u001B[32m   1133\u001B[39m \u001B[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001B[39;00m\n\u001B[32m   1134\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1246\u001B[39m, in \u001B[36mSeries._get_value\u001B[39m\u001B[34m(self, label, takeable)\u001B[39m\n\u001B[32m   1243\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[label]\n\u001B[32m   1245\u001B[39m \u001B[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1246\u001B[39m loc = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1248\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_integer(loc):\n\u001B[32m   1249\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._values[loc]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:415\u001B[39m, in \u001B[36mRangeIndex.get_loc\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m    413\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._range.index(new_key)\n\u001B[32m    414\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[32m--> \u001B[39m\u001B[32m415\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01merr\u001B[39;00m\n\u001B[32m    416\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Hashable):\n\u001B[32m    417\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key)\n",
      "\u001B[31mKeyError\u001B[39m: -2"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset and DataLoader",
   "id": "f04c70046f9e7ad2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T08:42:05.757364Z",
     "start_time": "2025-08-27T08:42:05.740304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SASRecDataset(Dataset):\n",
    "    def __init__(self, user_sequences, all_train_sequences, num_items, max_seq_len=50, mode=\"train\", neg_samples=1):\n",
    "        self.user_sequences = user_sequences\n",
    "        self.all_train_sequences = all_train_sequences  # For negative sampling\n",
    "        self.num_items = num_items\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mode = mode\n",
    "        self.neg_samples = neg_samples\n",
    "\n",
    "        # Build user interaction history for negative sampling\n",
    "        self.user_item_set = defaultdict(set)\n",
    "        for u, items in all_train_sequences.items():\n",
    "            self.user_item_set[u] = set(items)\n",
    "\n",
    "        # Create samples based on mode\n",
    "        self.samples = []\n",
    "        for user, seq in user_sequences.items():\n",
    "            if mode == \"train\":\n",
    "                # Training: create multiple samples per sequence\n",
    "                for t in range(1, len(seq)):\n",
    "                    input_seq = seq[:t]\n",
    "                    target = seq[t]\n",
    "                    self.samples.append((user, input_seq, target))\n",
    "            elif mode == \"val\":\n",
    "                # Validation: predict the second-to-last item\n",
    "                if len(seq) < 2:\n",
    "                    continue\n",
    "                input_seq = seq[:-2]  # Use train sequence\n",
    "                target = seq[-2]      # Predict validation item\n",
    "                self.samples.append((user, input_seq, target))\n",
    "            else:  # test\n",
    "                # Test: predict the last item\n",
    "                input_seq = seq[:-1]  # Use train+val sequence\n",
    "                target = seq[-1]      # Predict test item\n",
    "                self.samples.append((user, input_seq, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user, input_seq, target = self.samples[idx]\n",
    "\n",
    "        # Truncate if too long\n",
    "        if len(input_seq) > self.max_seq_len:\n",
    "            input_seq = input_seq[-self.max_seq_len:]\n",
    "\n",
    "        # Pad sequence\n",
    "        pad_len = self.max_seq_len - len(input_seq)\n",
    "        input_seq = [0] * pad_len + input_seq\n",
    "\n",
    "        # Sample negatives (items not in user's training history)\n",
    "        neg_items = self._sample_negatives(user)\n",
    "\n",
    "        return {\n",
    "            \"user\": user,\n",
    "            \"input_seq\": torch.tensor(input_seq, dtype=torch.long),\n",
    "            \"target\": torch.tensor(target, dtype=torch.long),\n",
    "            \"neg_items\": torch.tensor(neg_items, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def _sample_negatives(self, user):\n",
    "        \"\"\"Sample items that user hasn't interacted with in training\"\"\"\n",
    "        neg_items = set()\n",
    "        user_items = self.user_item_set[user]\n",
    "        while len(neg_items) < self.neg_samples:\n",
    "            neg = random.randint(1, self.num_items - 1)\n",
    "            if neg not in user_items:\n",
    "                neg_items.add(neg)\n",
    "        return list(neg_items)"
   ],
   "id": "6b6e10207bb6d6e4",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T08:42:08.788092Z",
     "start_time": "2025-08-27T08:42:05.763548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create datasets with correct sequences\n",
    "NUM_ITEMS = int(df_encoded[\"item_id\"].max()) + 1\n",
    "\n",
    "train_dataset = SASRecDataset(\n",
    "    train_sequences, train_sequences, NUM_ITEMS,\n",
    "    max_seq_len=50, mode=\"train\", neg_samples=1\n",
    ")\n",
    "\n",
    "val_dataset = SASRecDataset(\n",
    "    val_sequences, train_sequences, NUM_ITEMS,\n",
    "    max_seq_len=50, mode=\"val\", neg_samples=99\n",
    ")\n",
    "\n",
    "test_dataset = SASRecDataset(\n",
    "    test_sequences, train_sequences, NUM_ITEMS,\n",
    "    max_seq_len=50, mode=\"test\", neg_samples=99\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2048, shuffle=False)"
   ],
   "id": "dcb03e0f52b1da7e",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T08:42:09.059200Z",
     "start_time": "2025-08-27T08:42:08.864243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "first = next(iter(val_loader))\n",
    "print(\"Sample batch from validation loader:\")\n",
    "print(\"Input sequence shape:\", first[\"input_seq\"].shape)\n",
    "print(\"Target shape:\", first[\"target\"].shape)\n",
    "print(\"Negative items shape:\", first[\"neg_items\"].shape)\n",
    "\n",
    "print(\"\\nSample input sequence:\")\n",
    "random_index = []\n",
    "for _ in range(10):\n",
    "    random_index.append(random.randint(0, len(val_loader)-1))\n",
    "\n",
    "for i in random_index:\n",
    "    print(first[\"input_seq\"][i])"
   ],
   "id": "9cf8d7b1e3a87a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch from validation loader:\n",
      "Input sequence shape: torch.Size([2048, 50])\n",
      "Target shape: torch.Size([2048])\n",
      "Negative items shape: torch.Size([2048, 99])\n",
      "\n",
      "Sample input sequence:\n",
      "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0, 734224])\n",
      "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "        917103, 554749, 545894, 545887, 545729])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0, 999328, 968497, 314542, 845644,\n",
      "        220666, 845743, 221805, 517123, 221763])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "        369979, 163678, 287339,  94002, 903610])\n",
      "tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0, 334068])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n",
      "tensor([      0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0,       0,       0,       0,       0,       0,\n",
      "              0,       0,       0, 1141449,  640445,  965128,  195722,  219900,\n",
      "         951023,   27485])\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create SASRec model",
   "id": "ee51311214a572b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T08:51:37.662422Z",
     "start_time": "2025-08-27T08:51:37.641884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SASRec(nn.Module):\n",
    "    def __init__(self, num_items, hidden_dim=50, max_seq_len=50,\n",
    "                 num_blocks=2, num_heads=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_items = num_items\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Embeddings\n",
    "        self.item_embed = nn.Embedding(num_items, hidden_dim, padding_idx=0)\n",
    "        self.pos_embed = nn.Embedding(max_seq_len, hidden_dim)\n",
    "        self.emb_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Attention blocks\n",
    "        self.attention_blocks = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'attention': nn.MultiheadAttention(\n",
    "                    hidden_dim, num_heads, dropout=dropout, batch_first=True\n",
    "                ),\n",
    "                'feed_forward': nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(dropout),\n",
    "                    nn.Linear(hidden_dim, hidden_dim),\n",
    "                    nn.Dropout(dropout)\n",
    "                ),\n",
    "                'ln1': nn.LayerNorm(hidden_dim),\n",
    "                'ln2': nn.LayerNorm(hidden_dim)\n",
    "            }) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        self.ln_out = nn.LayerNorm(hidden_dim)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Xavier initialization\n",
    "        for param in self.parameters():\n",
    "            if param.dim() > 1:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        batch_size, seq_len = input_seq.shape\n",
    "\n",
    "        # Get embeddings\n",
    "        seqs_emb = self.item_embed(input_seq)  # [B, L, D]\n",
    "        positions = torch.arange(seq_len, device=input_seq.device).expand(batch_size, -1)\n",
    "        pos_emb = self.pos_embed(positions)    # [B, L, D]\n",
    "\n",
    "        # Combine embeddings\n",
    "        emb = self.emb_dropout(seqs_emb + pos_emb)\n",
    "\n",
    "        # Create masks\n",
    "        padding_mask = (input_seq == 0)  # [B, L]\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(seq_len, seq_len, device=input_seq.device),\n",
    "            diagonal=1\n",
    "        ).bool()  # [L, L]\n",
    "\n",
    "        # Process through attention blocks\n",
    "        timeline_rep = emb\n",
    "        for block in self.attention_blocks:\n",
    "            # Multi-head attention\n",
    "            residual = timeline_rep\n",
    "            timeline_rep_norm = block['ln1'](timeline_rep)\n",
    "            attn_out, _ = block['attention'](\n",
    "                timeline_rep_norm, timeline_rep_norm, timeline_rep_norm,\n",
    "                key_padding_mask=padding_mask,\n",
    "                attn_mask=causal_mask\n",
    "            )\n",
    "            timeline_rep = residual + attn_out\n",
    "\n",
    "            # Feed-forward\n",
    "            residual = timeline_rep\n",
    "            timeline_rep_norm = block['ln2'](timeline_rep)\n",
    "            ff_out = block['feed_forward'](timeline_rep_norm)\n",
    "            timeline_rep = residual + ff_out\n",
    "\n",
    "        # Output layer norm\n",
    "        output = self.ln_out(timeline_rep)  # [B, L, D]\n",
    "        return output\n",
    "\n",
    "    def predict(self, input_seq, candidates):\n",
    "        \"\"\"\n",
    "        Predict scores for candidate items\n",
    "        \"\"\"\n",
    "        seq_emb = self.forward(input_seq)  # [B, L, D]\n",
    "\n",
    "        # Use last non-padding position\n",
    "        mask = (input_seq != 0).float()\n",
    "        last_pos = mask.sum(1).long() - 1  # [B]\n",
    "        last_pos = last_pos.clamp(min=0)\n",
    "\n",
    "        # Get representation at last position\n",
    "        batch_idx = torch.arange(input_seq.size(0), device=input_seq.device)\n",
    "        final_feat = seq_emb[batch_idx, last_pos]  # [B, D]\n",
    "\n",
    "        # Score candidates\n",
    "        cand_emb = self.item_embed(candidates)  # [B, N, D] or [B, D]\n",
    "\n",
    "        if candidates.dim() == 2:  # Multiple candidates per user\n",
    "            scores = torch.matmul(cand_emb, final_feat.unsqueeze(-1)).squeeze(-1)\n",
    "        else:  # Single candidate\n",
    "            scores = (cand_emb * final_feat).sum(dim=1)\n",
    "\n",
    "        return scores"
   ],
   "id": "6af96d98ee3a7051",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training and evaluation functions",
   "id": "126fb73d3965ec4f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T08:51:53.472493Z",
     "start_time": "2025-08-27T08:51:53.464517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_seq = batch[\"input_seq\"].to(device)\n",
    "        pos_items = batch[\"target\"].to(device)\n",
    "        neg_items = batch[\"neg_items\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        seq_output = model(input_seq)  # [B, L, D]\n",
    "\n",
    "        # Get last position output\n",
    "        batch_size = input_seq.size(0)\n",
    "        mask = (input_seq != 0).float()\n",
    "        last_pos = mask.sum(1).long() - 1\n",
    "        last_pos = last_pos.clamp(min=0)\n",
    "        batch_idx = torch.arange(batch_size, device=device)\n",
    "        final_feat = seq_output[batch_idx, last_pos]  # [B, D]\n",
    "\n",
    "        # Compute scores\n",
    "        pos_emb = model.item_embed(pos_items)  # [B, D]\n",
    "        neg_emb = model.item_embed(neg_items.squeeze())  # [B, D]\n",
    "\n",
    "        pos_scores = (final_feat * pos_emb).sum(dim=1)  # [B]\n",
    "        neg_scores = (final_feat * neg_emb).sum(dim=1)  # [B]\n",
    "\n",
    "        # BPR loss\n",
    "        loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-10).mean()\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)"
   ],
   "id": "89794d62607239d3",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T08:52:02.996550Z",
     "start_time": "2025-08-27T08:52:02.987552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def evaluate(model, eval_loader, k=10, device='cpu'):\n",
    "    model.eval()\n",
    "\n",
    "    hr_list = []\n",
    "    ndcg_list = []\n",
    "    mrr_list = []\n",
    "\n",
    "    for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
    "        input_seq = batch[\"input_seq\"].to(device)\n",
    "        target = batch[\"target\"].to(device)\n",
    "        neg_items = batch[\"neg_items\"].to(device)\n",
    "\n",
    "        batch_size = input_seq.size(0)\n",
    "\n",
    "        # Create candidate set: target + negatives\n",
    "        candidates = torch.cat([\n",
    "            target.unsqueeze(1),  # [B, 1]\n",
    "            neg_items  # [B, 99]\n",
    "        ], dim=1)  # [B, 100]\n",
    "\n",
    "        # Get predictions\n",
    "        scores = model.predict(input_seq, candidates)  # [B, 100]\n",
    "\n",
    "        # Get ranks (0 is the positive item)\n",
    "        _, indices = torch.topk(scores, k=min(k, scores.size(1)), dim=1)\n",
    "\n",
    "        # Calculate metrics\n",
    "        for i in range(batch_size):\n",
    "            # Check if positive item (index 0) is in top-k\n",
    "            if 0 in indices[i]:\n",
    "                hr_list.append(1.0)\n",
    "                rank = (indices[i] == 0).nonzero(as_tuple=True)[0].item() + 1\n",
    "                ndcg_list.append(1.0 / np.log2(rank + 1))\n",
    "                mrr_list.append(1.0 / rank)\n",
    "            else:\n",
    "                hr_list.append(0.0)\n",
    "                ndcg_list.append(0.0)\n",
    "                mrr_list.append(0.0)\n",
    "\n",
    "    return {\n",
    "        f\"HR@{k}\": np.mean(hr_list),\n",
    "        f\"NDCG@{k}\": np.mean(ndcg_list),\n",
    "        f\"MRR@{k}\": np.mean(mrr_list)\n",
    "    }"
   ],
   "id": "206c2ec3b34e4147",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the model",
   "id": "3aeff0d9d09d95c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:00:59.642576Z",
     "start_time": "2025-08-27T08:57:55.092331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "model = SASRec(\n",
    "    num_items=NUM_ITEMS,\n",
    "    hidden_dim=64,  # Original paper uses 50\n",
    "    max_seq_len=50,\n",
    "    num_blocks=2,\n",
    "    num_heads=2,    # Original paper uses 1\n",
    "    dropout=0.5\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "best_ndcg = 0\n",
    "patience = 10\n",
    "no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, None, DEVICE)\n",
    "\n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, k=10, device=DEVICE)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Metrics: {val_metrics}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_metrics[\"NDCG@10\"] > best_ndcg:\n",
    "        best_ndcg = val_metrics[\"NDCG@10\"]\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Load best model and test\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "test_metrics = evaluate(model, test_loader, k=10, device=DEVICE)\n",
    "print(f\"\\nTest Metrics: {test_metrics}\")"
   ],
   "id": "f80ad10395c89ee5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 178/178 [00:17<00:00, 10.43it/s]\n",
      "Evaluating: 100%|██████████| 74/74 [01:27<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Train Loss: nan\n",
      "Val Metrics: {'HR@10': np.float64(1.0), 'NDCG@10': np.float64(0.3562071871080221), 'MRR@10': np.float64(0.16666666666666657)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 178/178 [00:16<00:00, 10.63it/s]\n",
      "Evaluating:  62%|██████▏   | 46/74 [00:52<00:32,  1.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[36]\u001B[39m\u001B[32m, line 25\u001B[39m\n\u001B[32m     22\u001B[39m train_loss = train_epoch(model, train_loader, optimizer, \u001B[38;5;28;01mNone\u001B[39;00m, DEVICE)\n\u001B[32m     24\u001B[39m \u001B[38;5;66;03m# Evaluate\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m25\u001B[39m val_metrics = \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     28\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTrain Loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    119\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[34]\u001B[39m\u001B[32m, line 32\u001B[39m, in \u001B[36mevaluate\u001B[39m\u001B[34m(model, eval_loader, k, device)\u001B[39m\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# Calculate metrics\u001B[39;00m\n\u001B[32m     30\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(batch_size):\n\u001B[32m     31\u001B[39m     \u001B[38;5;66;03m# Check if positive item (index 0) is in top-k\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m32\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[32;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m:\n\u001B[32m     33\u001B[39m         hr_list.append(\u001B[32m1.0\u001B[39m)\n\u001B[32m     34\u001B[39m         rank = (indices[i] == \u001B[32m0\u001B[39m).nonzero(as_tuple=\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[32m0\u001B[39m].item() + \u001B[32m1\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\torch\\_tensor.py:1255\u001B[39m, in \u001B[36mTensor.__contains__\u001B[39m\u001B[34m(self, element)\u001B[39m\n\u001B[32m   1250\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(Tensor.\u001B[34m__contains__\u001B[39m, (\u001B[38;5;28mself\u001B[39m,), \u001B[38;5;28mself\u001B[39m, element)\n\u001B[32m   1251\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\n\u001B[32m   1252\u001B[39m     element, (torch.Tensor, Number, torch.SymInt, torch.SymFloat, torch.SymBool)\n\u001B[32m   1253\u001B[39m ):\n\u001B[32m   1254\u001B[39m     \u001B[38;5;66;03m# type hint doesn't understand the __contains__ result array\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1255\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mbool\u001B[39m(\u001B[43m(\u001B[49m\u001B[43melement\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43many\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[32m   1257\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m   1258\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(element)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1259\u001B[39m )\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "52c46a03093c0707"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
