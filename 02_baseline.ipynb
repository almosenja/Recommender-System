{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Recommender System - Baseline Model\n",
    "This notebook implements a simple baseline which is content-based cosine similarity."
   ],
   "id": "7c822c18c3c50055"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:23:36.008232Z",
     "start_time": "2025-08-19T15:23:34.155849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ],
   "id": "5a6345db3eba7a39",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:23:36.034842Z",
     "start_time": "2025-08-19T15:23:36.013136Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
   "id": "2a1356b479a71c7c",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:23:36.326793Z",
     "start_time": "2025-08-19T15:23:36.043949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing processed dataset\n",
    "df = pd.read_csv(\"data/amazon_reviews_2023_filtered.csv\")\n",
    "df.head()"
   ],
   "id": "c2f3a4c83817f559",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                           user        item  rating         domain  \\\n",
       "0  AFERCDY2EFJKT7QUQ75GISNHTFOQ  0345409469     5.0          Books   \n",
       "1  AEAFMJT3QRZZEJ3CTGB4FNDL5FPA  0446606324     5.0          Books   \n",
       "2  AEAFMJT3QRZZEJ3CTGB4FNDL5FPA  0440212561     5.0          Books   \n",
       "3  AE2HHCZARXNN3PDMFANR7XK23LIA  0800141601     1.0  Movies_and_TV   \n",
       "4  AFTFGYARCCB72L37G6RBSPPJZ3RA  B00002EPYD     2.0    Video_Games   \n",
       "\n",
       "      timestamp  implicit_rating  user_id  item_id  domain_id  \n",
       "0  874018431000                1    12197      588          0  \n",
       "1  912129267000                1     1818     1004          0  \n",
       "2  933620912000                1     1818      976          0  \n",
       "3  939519163000                0      114     1894          1  \n",
       "4  949468790000                0    16286     3953          2  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>item</th>\n",
       "      <th>rating</th>\n",
       "      <th>domain</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>implicit_rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>domain_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFERCDY2EFJKT7QUQ75GISNHTFOQ</td>\n",
       "      <td>0345409469</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Books</td>\n",
       "      <td>874018431000</td>\n",
       "      <td>1</td>\n",
       "      <td>12197</td>\n",
       "      <td>588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEAFMJT3QRZZEJ3CTGB4FNDL5FPA</td>\n",
       "      <td>0446606324</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Books</td>\n",
       "      <td>912129267000</td>\n",
       "      <td>1</td>\n",
       "      <td>1818</td>\n",
       "      <td>1004</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEAFMJT3QRZZEJ3CTGB4FNDL5FPA</td>\n",
       "      <td>0440212561</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Books</td>\n",
       "      <td>933620912000</td>\n",
       "      <td>1</td>\n",
       "      <td>1818</td>\n",
       "      <td>976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE2HHCZARXNN3PDMFANR7XK23LIA</td>\n",
       "      <td>0800141601</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Movies_and_TV</td>\n",
       "      <td>939519163000</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>1894</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFTFGYARCCB72L37G6RBSPPJZ3RA</td>\n",
       "      <td>B00002EPYD</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Video_Games</td>\n",
       "      <td>949468790000</td>\n",
       "      <td>0</td>\n",
       "      <td>16286</td>\n",
       "      <td>3953</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Splitting the dataset chronologically and by user",
   "id": "f1783303bdad3402"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:23:40.134468Z",
     "start_time": "2025-08-19T15:23:39.812988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from recommenders.datasets.python_splitters import python_chrono_split, python_random_split\n",
    "\n",
    "train, temp = python_chrono_split(\n",
    "    df, ratio=0.8, filter_by=\"user\",\n",
    "    col_user=\"user_id\", col_item=\"item_id\", col_timestamp=\"timestamp\"\n",
    ")\n",
    "\n",
    "val, test = python_random_split(\n",
    "    temp, ratio=0.5\n",
    ")\n",
    "\n",
    "print(f\"Training dataset percentage: {(len(train) / len(df)) * 100}%\")\n",
    "print(f\"Validation dataset percentage: {(len(val) / len(df)) * 100}%\")\n",
    "print(f\"Test dataset percentage: {(len(test) / len(df)) * 100}%\")"
   ],
   "id": "681e166b4997ff09",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset percentage: 80.06679160231765%\n",
      "Validation dataset percentage: 9.966425036817796%\n",
      "Test dataset percentage: 9.966783360864564%\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T15:47:52.061465Z",
     "start_time": "2025-08-18T15:47:51.927542Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 78,
   "source": [
    "# # Splitting the datasets into train and test sets by user and time\n",
    "# def stratified_temporal_split(dataframe, val_ratio=0.1, test_ratio=0.1):\n",
    "#     dataframe = dataframe.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "#     grouped = dataframe.groupby(\"user_id\")\n",
    "#\n",
    "#     train_indices, val_indices, test_indices = [], [], []\n",
    "#     for _, group in grouped:\n",
    "#         n_records = len(group)\n",
    "#         indices = group.index\n",
    "#\n",
    "#         # If user has less that 3 interactions, put all in training set\n",
    "#         if n_records < 3:\n",
    "#             train_indices.extend(indices)\n",
    "#             continue\n",
    "#\n",
    "#         # Calculate split points\n",
    "#         test_split_idx = int(n_records * (1 - test_ratio))\n",
    "#         val_split_idx = int(n_records * (1 - test_ratio - val_ratio))\n",
    "#\n",
    "#         # Ensure validation set has at least one record if percentages are non-zero\n",
    "#         if val_split_idx <= 0:\n",
    "#             val_split_idx = 1\n",
    "#\n",
    "#         # Assign indices to respective sets\n",
    "#         train_indices.extend(indices[:val_split_idx])\n",
    "#         val_indices.extend(indices[val_split_idx:test_split_idx])\n",
    "#         test_indices.extend(indices[test_split_idx:])\n",
    "#\n",
    "#     # Create the final datasets\n",
    "#     train_df = dataframe.loc[train_indices].sort_values(\"timestamp\")\n",
    "#     val_df = dataframe.loc[val_indices].sort_values(\"timestamp\")\n",
    "#     test_df = dataframe.loc[test_indices].sort_values(\"timestamp\")\n",
    "#\n",
    "#     return train_df, val_df, test_df\n",
    "#\n",
    "# train, val, test = stratified_temporal_split(df, val_ratio=0.2, test_ratio=0.1)"
   ],
   "id": "f94d443d32e11f9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating dataset and dataloader",
   "id": "3e4632593e86ae23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:03:55.581515Z",
     "start_time": "2025-08-19T15:03:55.575291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MFDataset(Dataset):\n",
    "    def __init__(self, data, all_item_ids, is_train=False, num_neg=4):\n",
    "        self.data = data\n",
    "        self.all_item_ids = all_item_ids\n",
    "        self.is_train = is_train\n",
    "        self.num_neg = num_neg if self.is_train else 0\n",
    "        self.user_item_pairs = set(zip(self.data[\"user_id\"], self.data[\"item_id\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * (1 + self.num_neg)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx\n",
    "        idx = idx // (1 + self.num_neg)\n",
    "        user = self.data[\"user_id\"].iloc[idx]\n",
    "        item = self.data[\"item_id\"].iloc[idx]\n",
    "\n",
    "        if self.is_train:\n",
    "            label = 1 if original_idx % (1 + self.num_neg) == 0 else 0\n",
    "            if label == 0:\n",
    "                neg_item = np.random.choice(self.all_item_ids)\n",
    "                while (user, neg_item) in self.user_item_pairs:\n",
    "                    neg_item = np.random.choice(self.all_item_ids)\n",
    "                item = neg_item\n",
    "        else:\n",
    "            label = float(self.data[\"implicit_rating\"].iloc[idx])\n",
    "\n",
    "        return torch.tensor(user, dtype=torch.long), torch.tensor(item, dtype=torch.long), torch.tensor(label, dtype=torch.float32)"
   ],
   "id": "40c143c070b535a3",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:23:46.120430Z",
     "start_time": "2025-08-19T15:23:46.113535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MFDataset(Dataset):\n",
    "    def __init__(self, data, all_item_ids, is_train=False, num_neg=4, include_neg_val=True):\n",
    "        self.data = data\n",
    "        self.all_item_ids = all_item_ids\n",
    "        self.is_train = is_train\n",
    "        self.include_neg_val = include_neg_val  # New parameter for validation negative sampling\n",
    "\n",
    "        # Determine negative sampling based on mode\n",
    "        if self.is_train:\n",
    "            self.num_neg = num_neg\n",
    "        elif self.include_neg_val and not self.is_train:\n",
    "            self.num_neg = 1  # Include 1 negative sample for validation/test\n",
    "        else:\n",
    "            self.num_neg = 0\n",
    "\n",
    "        self.user_item_pairs = set(zip(self.data[\"user_id\"], self.data[\"item_id\"]))\n",
    "\n",
    "        # Pre-compute user interactions for faster negative sampling\n",
    "        self.user_items = {}\n",
    "        for user, item in self.user_item_pairs:\n",
    "            if user not in self.user_items:\n",
    "                self.user_items[user] = set()\n",
    "            self.user_items[user].add(item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * (1 + self.num_neg)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_idx = idx\n",
    "        idx = idx // (1 + self.num_neg)\n",
    "        user = self.data[\"user_id\"].iloc[idx]\n",
    "        item = self.data[\"item_id\"].iloc[idx]\n",
    "\n",
    "        if self.num_neg > 0:  # If using negative sampling\n",
    "            label = 1 if original_idx % (1 + self.num_neg) == 0 else 0\n",
    "            if label == 0:\n",
    "                # More efficient negative sampling\n",
    "                neg_item = np.random.choice(self.all_item_ids)\n",
    "                while neg_item in self.user_items.get(user, set()):\n",
    "                    neg_item = np.random.choice(self.all_item_ids)\n",
    "                item = neg_item\n",
    "        else:\n",
    "            label = float(self.data[\"implicit_rating\"].iloc[idx])\n",
    "\n",
    "        return torch.tensor(user, dtype=torch.long), torch.tensor(item, dtype=torch.long), torch.tensor(label, dtype=torch.float32)"
   ],
   "id": "8cd48f95f850bed4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:32:07.789177Z",
     "start_time": "2025-08-19T15:32:07.783823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MFDataset(Dataset):\n",
    "    def __init__(self, data, all_item_ids, is_train=False, num_neg=4):\n",
    "        self.data = data\n",
    "        self.all_item_ids = all_item_ids\n",
    "        self.is_train = is_train\n",
    "        self.num_neg = num_neg\n",
    "        self.user_positives = data.groupby('user_id')['item_id'].apply(set).to_dict()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) * (1 + self.num_neg) if self.is_train else len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            user_idx = idx // (1 + self.num_neg)\n",
    "            user = self.data[\"user_id\"].iloc[user_idx]\n",
    "            item = self.data[\"item_id\"].iloc[user_idx]\n",
    "\n",
    "            if idx % (1 + self.num_neg) == 0:\n",
    "                label = 1.0\n",
    "            else:\n",
    "                label = 0.0\n",
    "                positive_items = self.user_positives.get(user, set())\n",
    "                item = np.random.choice(self.all_item_ids)\n",
    "                while item in positive_items:\n",
    "                    item = np.random.choice(self.all_item_ids)\n",
    "        else:\n",
    "            user = self.data[\"user_id\"].iloc[idx]\n",
    "            item = self.data[\"item_id\"].iloc[idx]\n",
    "            label = float(self.data[\"implicit_rating\"].iloc[idx])\n",
    "\n",
    "        return torch.tensor(user, dtype=torch.long), torch.tensor(item, dtype=torch.long), torch.tensor(label, dtype=torch.float32)"
   ],
   "id": "c5a9489ad36f8e93",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:32:09.441173Z",
     "start_time": "2025-08-19T15:32:08.523459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_item_ids = np.unique(df[\"item_id\"])\n",
    "\n",
    "train_ds = MFDataset(train, all_item_ids, is_train=True, num_neg=4)\n",
    "val_ds = MFDataset(val, all_item_ids, is_train=False)\n",
    "test_ds = MFDataset(test, all_item_ids, is_train=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=4096, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, batch_size=4096, shuffle=False)\n",
    "test_dataloader = DataLoader(test_ds, batch_size=4096, shuffle=False)"
   ],
   "id": "12f682e9ed3f8d57",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:34:13.261238Z",
     "start_time": "2025-08-19T15:34:13.122788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ub, ib, lb = next(iter(test_dataloader))\n",
    "print(ub)\n",
    "print(\"\\n\")\n",
    "print(ib)\n",
    "print(\"\\n\")\n",
    "print(np.unique_counts(lb))"
   ],
   "id": "1cd3971d694ab073",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([35180, 23714, 24932,  ...,  4889, 20470,  3891])\n",
      "\n",
      "\n",
      "tensor([24382, 26106,  7448,  ..., 27040,  7879,  8356])\n",
      "\n",
      "\n",
      "UniqueCountsResult(values=array([0., 1.], dtype=float32), counts=array([ 781, 3315]))\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:24:27.073335Z",
     "start_time": "2025-08-19T14:24:26.888636Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 103,
   "source": [
    "# ### TRY WITH DIFFERENT APPROACH\n",
    "# class MFDatasetV2(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df[[\"user_id\", \"item_id\", \"implicit_rating\"]]\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         return list(self.df.iloc[idx])\n",
    "#\n",
    "# # Split the dataset\n",
    "# train, val = python_chrono_split(\n",
    "#     df, ratio=0.9, filter_by=\"user\",\n",
    "#     col_user=\"user_id\", col_item=\"item_id\", col_timestamp=\"timestamp\"\n",
    "# )\n",
    "#\n",
    "# train_ds = MFDatasetV2(train)\n",
    "# val_ds = MFDatasetV2(val)\n",
    "#\n",
    "# train_dataloader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_ds, batch_size=128, shuffle=False)"
   ],
   "id": "9ebc24e72bfd3bcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:24:28.757843Z",
     "start_time": "2025-08-19T14:24:28.732668Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([16502, 23185, 12225,  1373, 25865, 33463, 19502, 23621, 33698, 29719,\n",
      "        11128, 25724, 15476, 29323, 17185,  5056, 20964, 34612, 17988, 24759,\n",
      "        10800,  7807, 22101,  9254, 27756,  4480,  3495, 19864, 12376, 30971,\n",
      "        29056,  6054, 13898, 34624, 13640,  2279,  3144,  9160, 22642, 28481,\n",
      "        31769, 16833, 16703,  6778, 12110, 11448,  7765, 16974, 30431,   761,\n",
      "         1840,   731, 24754, 35819, 24174,   656,  1326, 14519, 34281, 22202,\n",
      "         6371, 15507, 14158,  2653,  8423, 30071, 34556, 19799, 18132, 31736,\n",
      "        19323, 15548,  2984, 16084, 22410, 33688, 29555,   332, 18537, 23425,\n",
      "        11104, 12959, 16168, 27987,   555,  1854, 28824, 19777,  8945,  2551,\n",
      "        31418, 12150, 13975,  2084, 11873, 17871, 19337, 21398, 21616,   231,\n",
      "        10829,  9674,  6075,  6288, 12295,  7375, 19882, 34695, 30755,  7136,\n",
      "         5865, 20922,  1743, 35113, 15657,  5272, 18015, 23059, 29833, 11389,\n",
      "        20522,   391, 25758, 15829, 17956,  9923, 12713, 34723]), tensor([12822,   254,  6415,  2675, 27580, 18204,  6582, 23814, 23873, 17932,\n",
      "        18395,  2989,  4455,  9071, 14041, 20467, 24150, 12941,  6875, 12638,\n",
      "        11532, 10427, 18900, 16962, 14542,  9883,  6673, 16857, 10824, 24871,\n",
      "        26540, 18945, 27198, 10820, 23033,  9377, 12638,  9881, 17675,  6377,\n",
      "        20740, 20971, 16026, 17844, 18492, 20242,  2182,   310, 11786, 13366,\n",
      "        14972,   149,  6667,  3301,  1486,  5540, 21669, 22144,  9241, 13420,\n",
      "         5137, 24762, 14836,  2567, 19683, 17205, 15225, 12069,  8504, 17994,\n",
      "        13196,  8889, 19397,  8234, 17278, 23919, 25980, 25891, 12117,  2068,\n",
      "        14307, 25273, 27142, 20027, 12193, 16524, 15687,  8959,  8290, 18534,\n",
      "         1410,  1403, 23736, 20953, 21703, 14007,  8766, 12409, 17279, 12981,\n",
      "        26710, 20888, 24377, 26987, 17763, 10991,  5733, 12664, 17900, 28502,\n",
      "         1209,  5805, 11605, 14606, 15691, 16808,  9462,  8165, 14865, 17786,\n",
      "        26932, 20099,  2062, 10965,  4204,  2769, 23435, 24871]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1])]\n"
     ]
    }
   ],
   "execution_count": 104,
   "source": [
    "# ub = next(iter(train_dataloader))\n",
    "# print(ub)"
   ],
   "id": "35c680f19f3318d5"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-19T15:34:27.115951Z",
     "start_time": "2025-08-19T15:34:27.112029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Simple matrix factorization\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        return (self.user_embedding(user_ids) * self.item_embedding(item_ids)).sum(-1)"
   ],
   "id": "cac94da7e7d53bcb",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T15:37:19.904390Z",
     "start_time": "2025-08-19T15:34:27.993518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up model, optimizer, and loss function\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_users = len(df[\"user_id\"].unique())\n",
    "n_items = len(df[\"item_id\"].unique())\n",
    "\n",
    "# Model hyperparameters\n",
    "embedding_dim = 32\n",
    "lr = 1e-3\n",
    "epochs = 10\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, embedding_dim=embedding_dim).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n",
    "pos_weight = torch.tensor([4.0], device=DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "epoch_train_losses, epoch_val_losses = [], []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # TRAINING\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "    for users, items, labels in pbar:\n",
    "        users, items, labels = users.to(DEVICE), items.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(users, items)\n",
    "        loss = loss_fn(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    epoch_train_losses.append(avg_train_loss)\n",
    "\n",
    "    # VALIDATION\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar_val = tqdm(val_dataloader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")\n",
    "        for users, items, labels in pbar_val:\n",
    "            users, items, labels = users.to(DEVICE), items.to(DEVICE), labels.to(DEVICE)\n",
    "            output = model(users, items)\n",
    "            loss = loss_fn(output, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = torch.sigmoid(output)\n",
    "            preds = (output > 0.5).float()\n",
    "            correct_preds += (preds == labels).sum().item()\n",
    "            total_preds += labels.size(0)\n",
    "            pbar_val.set_postfix({\"val_loss\": loss.item()})\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    val_accuracy = correct_preds / total_preds\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"/n/nTraining completed.\")"
   ],
   "id": "930ccd233b27de8a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 273/273 [00:57<00:00,  4.78it/s, loss=1.12]\n",
      "Epoch 1/10 [Val]: 100%|██████████| 7/7 [00:01<00:00,  6.67it/s, val_loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 1.1090, Val Loss: 2.3822, Val Accuracy: 0.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 273/273 [00:56<00:00,  4.82it/s, loss=1.14]\n",
      "Epoch 2/10 [Val]: 100%|██████████| 7/7 [00:01<00:00,  6.77it/s, val_loss=2.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 - Train Loss: 1.1091, Val Loss: 2.3822, Val Accuracy: 0.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]:  98%|█████████▊| 268/273 [00:55<00:01,  4.79it/s, loss=1.12]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[27]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     23\u001B[39m train_loss = \u001B[32m0.0\u001B[39m\n\u001B[32m     25\u001B[39m pbar = tqdm(train_dataloader, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch+\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m [Train]\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43musers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitems\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpbar\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43musers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitems\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43musers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mitems\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python Projects\\recommendation_system\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001B[39m, in \u001B[36mtqdm.__iter__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1178\u001B[39m time = \u001B[38;5;28mself\u001B[39m._time\n\u001B[32m   1180\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1181\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m   1182\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\n\u001B[32m   1183\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Update and possibly print the progressbar.\u001B[39;49;00m\n\u001B[32m   1184\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;49;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python Projects\\recommendation_system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    731\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    732\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    733\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m734\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    735\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    736\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    737\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    738\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    739\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    740\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python Projects\\recommendation_system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    788\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    789\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m790\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    791\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    792\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\Python Projects\\recommendation_system\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m         data = \u001B[38;5;28mself\u001B[39m.dataset.__getitems__(possibly_batched_index)\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[32m     53\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m     data = \u001B[38;5;28mself\u001B[39m.dataset[possibly_batched_index]\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 31\u001B[39m, in \u001B[36mMFDataset.__getitem__\u001B[39m\u001B[34m(self, idx)\u001B[39m\n\u001B[32m     28\u001B[39m     item = \u001B[38;5;28mself\u001B[39m.data[\u001B[33m\"\u001B[39m\u001B[33mitem_id\u001B[39m\u001B[33m\"\u001B[39m].iloc[idx]\n\u001B[32m     29\u001B[39m     label = \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28mself\u001B[39m.data[\u001B[33m\"\u001B[39m\u001B[33mimplicit_rating\u001B[39m\u001B[33m\"\u001B[39m].iloc[idx])\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m torch.tensor(user, dtype=torch.long), \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlong\u001B[49m\u001B[43m)\u001B[49m, torch.tensor(label, dtype=torch.float32)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "43c443f2b605b13f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T14:25:23.136027Z",
     "start_time": "2025-08-19T14:24:34.866785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Training the model\n",
    "# n_users = len(df[\"user_id\"].unique())\n",
    "# n_items = len(df[\"item_id\"].unique())\n",
    "#\n",
    "# # Hyperparameters\n",
    "# EPOCHS = 50\n",
    "# LEARNING_RATE = 0.005\n",
    "#\n",
    "# model = MatrixFactorization(n_users, n_items, embedding_dim=32).to(DEVICE)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# epoch_train_losses, epoch_val_losses = [], []\n",
    "#\n",
    "# for i in range(EPOCHS):\n",
    "#     train_losses = []\n",
    "#     model.train()\n",
    "#\n",
    "#     pbar = tqdm(train_dataloader, desc=f\"Epoch {i+1}/{EPOCHS} [Train]\")\n",
    "#     for users, items, ratings in train_dataloader:\n",
    "#         users, items, ratings = users.to(DEVICE, dtype=torch.long), items.to(DEVICE, dtype=torch.long), ratings.to(DEVICE, dtype=torch.float32)\n",
    "#         optimizer.zero_grad()\n",
    "#         preds = model(users, items).reshape(-1)\n",
    "#         loss = loss_fn(preds, ratings)\n",
    "#         train_losses.append(loss.item())\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#\n",
    "#     model.eval()\n",
    "#     val_losses = []\n",
    "#     with torch.no_grad():\n",
    "#         for users, items, ratings in val_dataloader:\n",
    "#             users, items, ratings = users.to(DEVICE, dtype=torch.long), items.to(DEVICE, dtype=torch.long), ratings.to(DEVICE, dtype=torch.float32)\n",
    "#             preds = model(users, items).reshape(-1)\n",
    "#             loss = loss_fn(preds, ratings)\n",
    "#             val_losses.append(loss.item())\n",
    "#\n",
    "#     # Start logging\n",
    "#     epoch_train_loss = np.mean(train_losses)\n",
    "#     epoch_val_loss = np.mean(val_losses)\n",
    "#     epoch_train_losses.append(epoch_train_loss)\n",
    "#     epoch_val_losses.append(epoch_val_loss)\n",
    "#     print(f\"Epoch {i+1}/{EPOCHS} - Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}\")"
   ],
   "id": "7fbcd0042ec3bc13",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 - Train Loss: 0.6758, Val Loss: 0.6337\n",
      "Epoch 2/50 - Train Loss: 0.3072, Val Loss: 0.5899\n",
      "Epoch 3/50 - Train Loss: 0.0549, Val Loss: 0.6442\n",
      "Epoch 4/50 - Train Loss: 0.0130, Val Loss: 0.7055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[105]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     16\u001B[39m model.train()\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m users, items, ratings \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     users, items, ratings = \u001B[43musers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlong\u001B[49m\u001B[43m)\u001B[49m, items.to(DEVICE, dtype=torch.long), ratings.to(DEVICE, dtype=torch.float32)\n\u001B[32m     19\u001B[39m     optimizer.zero_grad()\n\u001B[32m     20\u001B[39m     preds = model(users, items).reshape(-\u001B[32m1\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "196a9071b33e7b1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
