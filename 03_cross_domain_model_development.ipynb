{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cross-Domain Recommendation System Development\n",
    "This notebook is an experiment in building a cross-domain recommendation system using the Amazon Reviews dataset. It uses the best model from the single-domain experiments and extends it to handle multiple domains. The dataset is the same as in the single-domain experiments, but now will combine data from two different domains."
   ],
   "id": "4198e13e22dcfbe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T06:08:15.370121Z",
     "start_time": "2025-08-27T06:08:09.215853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = \"D:/Python Projects/recommendation_system\"\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/data\"\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/models\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"E:/Python Scripts/recsys\"\n",
    "os.environ['HF_DATASETS_CACHE'] = \"E:/Python Scripts/recsys/data\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"E:/Python Scripts/recsys/models\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_dataset, Features, Value\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter"
   ],
   "id": "a90f1882aaa2c8d8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python Scripts\\recsys\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T06:08:15.450334Z",
     "start_time": "2025-08-27T06:08:15.387067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)"
   ],
   "id": "103f3ee91cc72343",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:00:57.099887Z",
     "start_time": "2025-08-27T07:00:57.067842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HF_DATASET = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "\n",
    "def load_amazon_reviews(domain:str,\n",
    "                        save_dir:str = \"data\",\n",
    "                        max_items:int | None = None,\n",
    "                        seed:int = SEED) -> pd.DataFrame:\n",
    "\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = f\"{save_dir}/amazon_reviews_{domain}.csv\"\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File {filepath} not found. Downloading dataset for domain '{domain}'...\")\n",
    "        ds = load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "            f\"raw_review_{domain}\",\n",
    "            split=\"full\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Keep only needed columns\n",
    "        ds = ds.select_columns([\"user_id\", \"parent_asin\", \"rating\", \"timestamp\"])\n",
    "        ds = ds.rename_columns({\"user_id\": \"user\", \"parent_asin\": \"item\"})\n",
    "        ds = ds.cast(Features({\n",
    "            \"user\": Value(\"string\"),\n",
    "            \"item\": Value(\"string\"),\n",
    "            \"rating\": Value(\"float32\"),\n",
    "            \"timestamp\": Value(\"int64\"),\n",
    "        }))\n",
    "\n",
    "        # Convert to pandas (Arrow zero-copy where possible)\n",
    "        df = ds.to_pandas()\n",
    "        df.insert(3, \"domain\", domain)\n",
    "        df.to_csv(f\"{save_dir}/amazon_reviews_{domain}.csv\", index=False)\n",
    "        print(f\"Saved amazon_reviews_{domain}.csv to {save_dir}/\")\n",
    "\n",
    "    final_df = pd.read_csv(filepath)\n",
    "    # Random subset if max_items is set\n",
    "    if max_items is not None:\n",
    "        k = min(max_items, len(final_df))\n",
    "        final_df = final_df.sample(n=k, random_state=seed).reset_index(drop=True)\n",
    "    print(f\"Loaded {filepath} with {len(final_df)} rows.\")\n",
    "    return final_df\n",
    "\n",
    "def preprocess_dataset(df, min_user_interactions=5, min_item_interactions=5):\n",
    "    # Make it implicit\n",
    "    df[\"label\"] = 1.0\n",
    "    # Filter users and items with less than k interactions\n",
    "    user_counts = df[\"user\"].value_counts()\n",
    "    item_counts = df[\"item\"].value_counts()\n",
    "    valid_users = user_counts[user_counts >= min_user_interactions].index\n",
    "    valid_items = item_counts[item_counts >= min_item_interactions].index\n",
    "    df = df[df[\"user\"].isin(valid_users) & df[\"item\"].isin(valid_items)].copy()\n",
    "    print(\"After interactions filtering:\", len(df), \"rows,\", df[\"user\"].nunique(), \"users,\", df[\"item\"].nunique(), \"items\")\n",
    "    return df\n",
    "\n",
    "def label_encoder(df, shift_item_id=False):\n",
    "    user_enc = LabelEncoder()\n",
    "    item_enc = LabelEncoder()\n",
    "    domain_enc = LabelEncoder()\n",
    "    df[\"user_id\"] = user_enc.fit_transform(df[\"user\"])\n",
    "    df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
    "    if shift_item_id:\n",
    "        df[\"item_id\"] = df[\"item_id\"] + 1  # Shift item IDs by 1 to reserve 0 for padding if needed\n",
    "    df[\"domain_id\"] = domain_enc.fit_transform(df[\"domain\"])\n",
    "    return {\"encoded_df\": df,\n",
    "            \"user_encoder\": user_enc,\n",
    "            \"item_encoder\": item_enc,\n",
    "            \"domain_encoder\": domain_enc}"
   ],
   "id": "cb8211dfd856edfc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparing Combined Dataset",
   "id": "7472f8a800012d84"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-27T07:56:27.268393Z",
     "start_time": "2025-08-27T07:53:31.455061Z"
    }
   },
   "source": [
    "# New input\n",
    "SOURCE_DOMAIN = \"Books\"\n",
    "TARGET_DOMAIN = \"Movies_and_TV\"\n",
    "ALL_DOMAIN = [SOURCE_DOMAIN, TARGET_DOMAIN]\n",
    "\n",
    "# Loading data from multiple domains\n",
    "def load_multi_domain_data(domains, max_items_per_domain=None, seed=SEED):\n",
    "    all_dfs = []\n",
    "    print(f\"Combining data from domains: {domains}\")\n",
    "    for domain in domains:\n",
    "        df_domain = load_amazon_reviews(domain, max_items=max_items_per_domain, seed=seed)\n",
    "        print(f\"{domain} domain data shape: {df_domain.shape}\")\n",
    "        all_dfs.append(df_domain)\n",
    "    all_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    all_df.to_csv(\"data/amazon_reviews_combined.csv\", index=False)\n",
    "    print(f\"Total interactions across domains: {len(all_df)}\")\n",
    "    final_df = pd.read_csv(\"data/amazon_reviews_combined.csv\")\n",
    "    return final_df\n",
    "\n",
    "combined_df = load_multi_domain_data(ALL_DOMAIN, max_items_per_domain=3_000_000, seed=SEED)\n",
    "print(f\"Total interactions across domains: {len(combined_df)}\")\n",
    "\n",
    "# Preprocess the combined dataset\n",
    "filtered_combined_df = preprocess_dataset(combined_df, min_user_interactions=20, min_item_interactions=20)\n",
    "\n",
    "le = label_encoder(filtered_combined_df, shift_item_id=True)\n",
    "combined_df_encoded = le[\"encoded_df\"]\n",
    "user_encoder = le[\"user_encoder\"]\n",
    "item_encoder = le[\"item_encoder\"]\n",
    "domain_encoder = le[\"domain_encoder\"]\n",
    "\n",
    "df_source = combined_df_encoded[combined_df_encoded[\"domain\"]== SOURCE_DOMAIN]\n",
    "df_target = combined_df_encoded[combined_df_encoded[\"domain\"] == TARGET_DOMAIN]\n",
    "\n",
    "NUM_USERS_ALL = combined_df_encoded[\"user_id\"].max() + 1\n",
    "NUM_ITEMS_ALL = combined_df_encoded[\"item_id\"].max() + 1\n",
    "NUM_DOMAINS = combined_df_encoded[\"domain_id\"].max() + 1\n",
    "print(f\"Number of all users: {NUM_USERS_ALL}, all items: {NUM_ITEMS_ALL}, all domains: {NUM_DOMAINS}\")\n",
    "\n",
    "NUM_USERS_SOURCE = df_source[\"user_id\"].max() + 1\n",
    "NUM_ITEMS_SOURCE = df_source[\"item_id\"].max() + 1\n",
    "NUM_USERS_TARGET = df_target[\"user_id\"].max() + 1\n",
    "NUM_ITEMS_TARGET = df_target[\"item_id\"].max() + 1\n",
    "print(f\"Source domain - users: {NUM_USERS_SOURCE}, items: {NUM_ITEMS_SOURCE}\")\n",
    "print(f\"Target domain - users: {NUM_USERS_TARGET}, items: {NUM_ITEMS_TARGET}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining data from domains: ['Books', 'Movies_and_TV']\n",
      "Loaded data/amazon_reviews_Books.csv with 3000000 rows.\n",
      "Books domain data shape: (3000000, 5)\n",
      "Loaded data/amazon_reviews_Movies_and_TV.csv with 3000000 rows.\n",
      "Movies_and_TV domain data shape: (3000000, 5)\n",
      "Total interactions across domains: 6000000\n",
      "Total interactions across domains: 6000000\n",
      "After interactions filtering: 72182 rows, 7520 users, 25209 items\n",
      "Number of all users: 7520, all items: 25210, all domains: 2\n",
      "Source domain - users: 7520, items: 25210\n",
      "Target domain - users: 7519, items: 25205\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:56:27.335935Z",
     "start_time": "2025-08-27T07:56:27.317922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_overlapping_users(df, domain1, domain2):\n",
    "    # 1. Get the unique set of users for each domain\n",
    "    users_in_domain1 = set(df[df[\"domain\"] == domain1][\"user\"].unique())\n",
    "    users_in_domain2 = set(df[df[\"domain\"] == domain2][\"user\"].unique())\n",
    "\n",
    "    # 2. Find the intersection of the two sets to get overlapping users\n",
    "    overlapping_users = users_in_domain1.intersection(users_in_domain2)\n",
    "\n",
    "    # --- Reporting Statistics ---\n",
    "    num_domain1 = len(users_in_domain1)\n",
    "    num_domain2 = len(users_in_domain2)\n",
    "    num_overlap = len(overlapping_users)\n",
    "\n",
    "    if num_domain1 == 0 or num_domain2 == 0:\n",
    "        print(\"Warning: One or both domains have no users in the dataframe.\")\n",
    "        return set()\n",
    "\n",
    "    print(f\"Total unique users in '{domain1}': {num_domain1}\")\n",
    "    print(f\"Total unique users in '{domain2}': {num_domain2}\")\n",
    "    print(f\"Number of users active in BOTH domains: {num_overlap}\")\n",
    "\n",
    "    # 3. Calculate overlap percentage\n",
    "    overlap_pct_d1 = (num_overlap / num_domain1) * 100\n",
    "    overlap_pct_d2 = (num_overlap / num_domain2) * 100\n",
    "    print(f\"These overlapping users represent {overlap_pct_d1:.2f}% of the '{domain1}' user base.\")\n",
    "    print(f\"These overlapping users represent {overlap_pct_d2:.2f}% of the '{domain2}' user base.\")\n",
    "\n",
    "    return overlapping_users"
   ],
   "id": "118a92928147b4c5",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T07:56:27.397770Z",
     "start_time": "2025-08-27T07:56:27.349980Z"
    }
   },
   "cell_type": "code",
   "source": "active_in_both = find_overlapping_users(combined_df_encoded, SOURCE_DOMAIN, TARGET_DOMAIN)",
   "id": "c32f35293c75c347",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique users in 'Books': 4256\n",
      "Total unique users in 'Movies_and_TV': 5604\n",
      "Number of users active in BOTH domains: 2340\n",
      "These overlapping users represent 54.98% of the 'Books' user base.\n",
      "These overlapping users represent 41.76% of the 'Movies_and_TV' user base.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "22ce50e9b7b83585"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
