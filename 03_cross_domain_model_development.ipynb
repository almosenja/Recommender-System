{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Cross-Domain Recommendation System Development\n",
    "This notebook is an experiment in building a cross-domain recommendation system using the Amazon Reviews dataset. It uses the best model from the single-domain experiments and extends it to handle multiple domains. The dataset is the same as in the single-domain experiments, but now will combine data from two different domains."
   ],
   "id": "4198e13e22dcfbe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:20:11.895372Z",
     "start_time": "2025-08-27T15:20:06.259591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/Python Projects/recommendation_system\"\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/data\"\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/models\"\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = \"E:/Python Scripts/recsys\"\n",
    "# os.environ['HF_DATASETS_CACHE'] = \"E:/Python Scripts/recsys/data\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = \"E:/Python Scripts/recsys/models\"\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_dataset, Features, Value\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter"
   ],
   "id": "a90f1882aaa2c8d8",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:20:11.934831Z",
     "start_time": "2025-08-27T15:20:11.903297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)"
   ],
   "id": "103f3ee91cc72343",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:20:11.948079Z",
     "start_time": "2025-08-27T15:20:11.939707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "HF_DATASET = \"McAuley-Lab/Amazon-Reviews-2023\"\n",
    "\n",
    "def load_amazon_reviews(domain, save_dir=\"data\", max_items=None, seed=SEED):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filepath = f\"{save_dir}/amazon_reviews_{domain}.csv\"\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File {filepath} not found. Downloading dataset for domain '{domain}'...\")\n",
    "        ds = load_dataset(\n",
    "            \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "            f\"raw_review_{domain}\",\n",
    "            split=\"full\",\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "        # Keep only needed columns\n",
    "        ds = ds.select_columns([\"user_id\", \"parent_asin\", \"rating\", \"timestamp\"])\n",
    "        ds = ds.rename_columns({\"user_id\": \"user\", \"parent_asin\": \"item\"})\n",
    "        ds = ds.cast(Features({\n",
    "            \"user\": Value(\"string\"),\n",
    "            \"item\": Value(\"string\"),\n",
    "            \"rating\": Value(\"float32\"),\n",
    "            \"timestamp\": Value(\"int64\"),\n",
    "        }))\n",
    "\n",
    "        # Convert to pandas (Arrow zero-copy where possible)\n",
    "        df = ds.to_pandas()\n",
    "        df.insert(3, \"domain\", domain)\n",
    "        df.to_csv(f\"{save_dir}/amazon_reviews_{domain}.csv\", index=False)\n",
    "        print(f\"Saved amazon_reviews_{domain}.csv to {save_dir}/\")\n",
    "\n",
    "    final_df = pd.read_csv(filepath)\n",
    "    if max_items is not None:\n",
    "        k = min(max_items, len(final_df))\n",
    "        final_df = final_df.sample(n=k, random_state=seed).reset_index(drop=True)\n",
    "    print(f\"Loaded {filepath} with {len(final_df)} rows.\")\n",
    "    return final_df\n",
    "\n",
    "def preprocess_dataset(df, min_user_interactions=5, min_item_interactions=5):\n",
    "    # Make it implicit\n",
    "    df[\"label\"] = 1.0\n",
    "    user_counts = df.groupby(\"user\").size()\n",
    "    valid_users = user_counts[user_counts >= min_user_interactions].index\n",
    "    item_counts = df.groupby(\"item\").size()\n",
    "    valid_items = item_counts[item_counts >= min_item_interactions].index\n",
    "    df_filtered = df[df[\"user\"].isin(valid_users) & df[\"item\"].isin(valid_items)]\n",
    "    print(\"After interactions filtering:\", len(df_filtered), \"rows,\", df_filtered[\"user\"].nunique(), \"users,\", df_filtered[\"item\"].nunique(), \"items\")\n",
    "    return df_filtered\n",
    "\n",
    "def label_encoder(df, shift_item_id=False):\n",
    "    user_enc = LabelEncoder()\n",
    "    item_enc = LabelEncoder()\n",
    "    domain_enc = LabelEncoder()\n",
    "    df[\"user_id\"] = user_enc.fit_transform(df[\"user\"])\n",
    "    df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
    "    if shift_item_id:\n",
    "        df[\"item_id\"] = df[\"item_id\"] + 1\n",
    "    df[\"domain_id\"] = domain_enc.fit_transform(df[\"domain\"])\n",
    "    return df, user_enc, item_enc, domain_enc"
   ],
   "id": "cb8211dfd856edfc",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparing Combined Dataset",
   "id": "7472f8a800012d84"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-27T15:23:08.732865Z",
     "start_time": "2025-08-27T15:20:11.955165Z"
    }
   },
   "source": [
    "# New input\n",
    "SOURCE_DOMAIN = \"Books\"\n",
    "TARGET_DOMAIN = \"Movies_and_TV\"\n",
    "ALL_DOMAIN = [SOURCE_DOMAIN, TARGET_DOMAIN]\n",
    "\n",
    "# Loading data from multiple domains\n",
    "def load_multi_domain_data(domains, max_items_per_domain=None, seed=SEED):\n",
    "    all_dfs = []\n",
    "    print(f\"Combining data from domains: {domains}\")\n",
    "    for domain in domains:\n",
    "        df_domain = load_amazon_reviews(domain, max_items=max_items_per_domain, seed=seed)\n",
    "        print(f\"{domain} domain data shape: {df_domain.shape}\")\n",
    "        all_dfs.append(df_domain)\n",
    "    all_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    all_df.to_csv(\"data/amazon_reviews_combined.csv\", index=False)\n",
    "    print(f\"Total interactions across domains: {len(all_df)}\")\n",
    "    final_df = pd.read_csv(\"data/amazon_reviews_combined.csv\")\n",
    "    return final_df\n",
    "\n",
    "combined_df = load_multi_domain_data(ALL_DOMAIN, max_items_per_domain=3_000_000, seed=SEED)\n",
    "print(f\"\\nTotal interactions across domains: {len(combined_df)}\")\n",
    "\n",
    "# Preprocess the combined dataset\n",
    "filtered_combined_df = preprocess_dataset(combined_df, min_user_interactions=20, min_item_interactions=20)\n",
    "combined_df_encoded, user_encoder, item_encoder, domain_encoder = label_encoder(filtered_combined_df, shift_item_id=True)\n",
    "\n",
    "df_source = combined_df_encoded[combined_df_encoded[\"domain\"]== SOURCE_DOMAIN]\n",
    "df_target = combined_df_encoded[combined_df_encoded[\"domain\"] == TARGET_DOMAIN]\n",
    "\n",
    "NUM_USERS_ALL = combined_df_encoded[\"user_id\"].max() + 1\n",
    "NUM_ITEMS_ALL = combined_df_encoded[\"item_id\"].max() + 1\n",
    "NUM_DOMAINS = combined_df_encoded[\"domain_id\"].max() + 1\n",
    "print(f\"\\nNumber of all users: {NUM_USERS_ALL}, all items: {NUM_ITEMS_ALL}, all domains: {NUM_DOMAINS}\")\n",
    "\n",
    "NUM_USERS_SOURCE = df_source[\"user_id\"].max() + 1\n",
    "NUM_ITEMS_SOURCE = df_source[\"item_id\"].max() + 1\n",
    "NUM_USERS_TARGET = df_target[\"user_id\"].max() + 1\n",
    "NUM_ITEMS_TARGET = df_target[\"item_id\"].max() + 1\n",
    "print(f\"\\nSource domain - users: {NUM_USERS_SOURCE}, items: {NUM_ITEMS_SOURCE}\")\n",
    "print(f\"Target domain - users: {NUM_USERS_TARGET}, items: {NUM_ITEMS_TARGET}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining data from domains: ['Books', 'Movies_and_TV']\n",
      "Loaded data/amazon_reviews_Books.csv with 3000000 rows.\n",
      "Books domain data shape: (3000000, 5)\n",
      "File data/amazon_reviews_Movies_and_TV.csv not found. Downloading dataset for domain 'Movies_and_TV'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/17328314 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f10dc09f5bd047e9acce2cd238af024b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved amazon_reviews_Movies_and_TV.csv to data/\n",
      "Loaded data/amazon_reviews_Movies_and_TV.csv with 3000000 rows.\n",
      "Movies_and_TV domain data shape: (3000000, 5)\n",
      "Total interactions across domains: 6000000\n",
      "\n",
      "Total interactions across domains: 6000000\n",
      "After interactions filtering: 72182 rows, 7520 users, 25209 items\n",
      "\n",
      "Number of all users: 7520, all items: 25210, all domains: 2\n",
      "\n",
      "Source domain - users: 7520, items: 25210\n",
      "Target domain - users: 7519, items: 25205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_18768\\2868909045.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"user_id\"] = user_enc.fit_transform(df[\"user\"])\n",
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_18768\\2868909045.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_18768\\2868909045.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"item_id\"] = df[\"item_id\"] + 1\n",
      "C:\\Users\\kulin\\AppData\\Local\\Temp\\ipykernel_18768\\2868909045.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"domain_id\"] = domain_enc.fit_transform(df[\"domain\"])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:23:08.766278Z",
     "start_time": "2025-08-27T15:23:08.761455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_overlapping_users(df, domain1, domain2):\n",
    "    # 1. Get the unique set of users for each domain\n",
    "    users_in_domain1 = set(df[df[\"domain\"] == domain1][\"user\"].unique())\n",
    "    users_in_domain2 = set(df[df[\"domain\"] == domain2][\"user\"].unique())\n",
    "\n",
    "    # 2. Find the intersection of the two sets to get overlapping users\n",
    "    overlapping_users = users_in_domain1.intersection(users_in_domain2)\n",
    "\n",
    "    # --- Reporting Statistics ---\n",
    "    num_domain1 = len(users_in_domain1)\n",
    "    num_domain2 = len(users_in_domain2)\n",
    "    num_overlap = len(overlapping_users)\n",
    "\n",
    "    if num_domain1 == 0 or num_domain2 == 0:\n",
    "        print(\"Warning: One or both domains have no users in the dataframe.\")\n",
    "        return set()\n",
    "\n",
    "    print(f\"Total unique users in '{domain1}': {num_domain1}\")\n",
    "    print(f\"Total unique users in '{domain2}': {num_domain2}\")\n",
    "    print(f\"Number of users active in BOTH domains: {num_overlap}\")\n",
    "\n",
    "    # 3. Calculate overlap percentage\n",
    "    overlap_pct_d1 = (num_overlap / num_domain1) * 100\n",
    "    overlap_pct_d2 = (num_overlap / num_domain2) * 100\n",
    "    print(f\"These overlapping users represent {overlap_pct_d1:.2f}% of the '{domain1}' user base.\")\n",
    "    print(f\"These overlapping users represent {overlap_pct_d2:.2f}% of the '{domain2}' user base.\")\n",
    "\n",
    "    return overlapping_users"
   ],
   "id": "118a92928147b4c5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:23:08.825489Z",
     "start_time": "2025-08-27T15:23:08.784331Z"
    }
   },
   "cell_type": "code",
   "source": "active_in_both = find_overlapping_users(combined_df_encoded, SOURCE_DOMAIN, TARGET_DOMAIN)",
   "id": "c32f35293c75c347",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique users in 'Books': 4256\n",
      "Total unique users in 'Movies_and_TV': 5604\n",
      "Number of users active in BOTH domains: 2340\n",
      "These overlapping users represent 54.98% of the 'Books' user base.\n",
      "These overlapping users represent 41.76% of the 'Movies_and_TV' user base.\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:33:13.886485Z",
     "start_time": "2025-08-27T15:33:13.498726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_user_sequences(df):\n",
    "    df_sorted = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "    user_sequences = {}\n",
    "    for uid, group in df_sorted.groupby(\"user_id\"):\n",
    "        items = group[\"item_id\"].tolist()\n",
    "        user_sequences[uid] = items\n",
    "\n",
    "    print(f\"Number of users: {len(user_sequences)}\")\n",
    "    print(f\"Max sequence length: {max(len(seq) for seq in user_sequences.values())}\")\n",
    "    print(f\"Min sequence length: {min(len(seq) for seq in user_sequences.values())}\")\n",
    "\n",
    "    return user_sequences\n",
    "\n",
    "user_sequences_src = create_user_sequences(df_source)\n",
    "user_sequences_tgt = create_user_sequences(df_target)\n",
    "pos_items_by_user_src = {u: set(seq) for u, seq in user_sequences_src.items()}\n",
    "pos_items_by_user_tgt = {u: set(seq) for u, seq in user_sequences_tgt.items()}"
   ],
   "id": "7444d0dd85e4deb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 4256\n",
      "Max sequence length: 41\n",
      "Min sequence length: 1\n",
      "Number of users: 5604\n",
      "Max sequence length: 170\n",
      "Min sequence length: 1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:34:33.278560Z",
     "start_time": "2025-08-27T15:34:33.105365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequences_loo_split(user_sequences):\n",
    "    train_seqs = {}\n",
    "    val_data = {}\n",
    "    test_data = {}\n",
    "\n",
    "    for user, seq in user_sequences.items():\n",
    "        if len(seq) < 3:  # Need at least 3 items for train/val/test\n",
    "            continue\n",
    "\n",
    "        train_seqs[user] = seq[:-2]  # All but last two\n",
    "        val_data[user] = (seq[:-2], seq[-2])  # Train on all but last 2, predict second-to-last\n",
    "        test_data[user] = (seq[:-1], seq[-1])  # Train on all but last, predict last\n",
    "\n",
    "    print(f\"Training sequences: {len(train_seqs)}\")\n",
    "    print(f\"Validation users: {len(val_data)}\")\n",
    "    print(f\"Test users: {len(test_data)}\")\n",
    "\n",
    "    return train_seqs, val_data, test_data\n",
    "\n",
    "train_seqs_src, val_data_src, test_data_src = sequences_loo_split(user_sequences_src)\n",
    "train_seqs_tgt, val_data_tgt, test_data_tgt = sequences_loo_split(user_sequences_tgt)\n",
    "print(f\"\\nSource Sequences - Train: {len(train_seqs_src)}, Val: {len(val_data_src)}, Test: {len(test_data_src)}\")\n",
    "print(f\"Target Sequences - Train: {len(train_seqs_tgt)}, Val: {len(val_data_tgt)}, Test: {len(test_data_tgt)}\")"
   ],
   "id": "cfb249298048503",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: 1946\n",
      "Validation users: 1946\n",
      "Test users: 1946\n",
      "Training sequences: 4602\n",
      "Validation users: 4602\n",
      "Test users: 4602\n",
      "\n",
      "Source Sequences - Train: 1946, Val: 1946, Test: 1946\n",
      "Target Sequences - Train: 4602, Val: 4602, Test: 4602\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compute user representations from sequences",
   "id": "63a2692ac89c6aeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:41:04.104406Z",
     "start_time": "2025-08-27T15:41:04.090172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SASRec model\n",
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.w2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(self.dropout(self.relu(self.w1(x))))\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # Layer norms\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.ffn = PointWiseFeedForward(hidden_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Self-attention with residual connection\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))\n",
    "\n",
    "        # Feed-forward network with residual connection\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + self.dropout(ffn_out))\n",
    "\n",
    "        return x\n",
    "\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_items,\n",
    "                 hidden_dim=64,\n",
    "                 max_seq_len=50,\n",
    "                 num_blocks=2,\n",
    "                 num_heads=2,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_items = num_items\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Embedding layers\n",
    "        self.item_embed = nn.Embedding(num_items, hidden_dim, padding_idx=0)\n",
    "        self.positional_embed = nn.Embedding(max_seq_len, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Stack of SASRec blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            AttentionBlock(hidden_dim, num_heads, dropout) for _ in range(num_blocks)\n",
    "        ])\n",
    "\n",
    "        # Final layer norm\n",
    "        self.ln = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.item_embed.weight[1:])  # Skip padding idx\n",
    "        nn.init.xavier_normal_(self.positional_embed.weight)\n",
    "\n",
    "    def forward(self, input_seq, candidate_items=None):\n",
    "        batch_size, seq_len = input_seq.shape\n",
    "\n",
    "        # Get item embeddings\n",
    "        item_embeds = self.item_embed(input_seq)  # [B, L, D]\n",
    "\n",
    "        # Add positional embeddings\n",
    "        positions = torch.arange(seq_len, device=input_seq.device).unsqueeze(0)\n",
    "        pos_embeds = self.positional_embed(positions)  # [1, L, D]\n",
    "        x = self.dropout(item_embeds + pos_embeds)\n",
    "\n",
    "        # Create causal attention mask\n",
    "        attn_mask = self._create_causal_mask(seq_len, input_seq.device)\n",
    "        pad_mask = input_seq.eq(0)\n",
    "\n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, attn_mask=attn_mask)\n",
    "\n",
    "        # Final layer norm\n",
    "        x = self.ln(x)  # [B, L, D]\n",
    "        x = x.masked_fill(pad_mask.unsqueeze(-1), 0.0)\n",
    "\n",
    "        # If candidate_items provided, score them\n",
    "        if candidate_items is not None:\n",
    "            # Get embeddings for candidate items\n",
    "            cand_emb = self.item_embed(candidate_items) # [B, N, D]\n",
    "\n",
    "            # Use last position's representation for scoring\n",
    "            last_hidden = x[:, -1, :].unsqueeze(1)  # [B, 1, D]\n",
    "\n",
    "            # Compute scores via dot product\n",
    "            scores = torch.matmul(last_hidden, cand_emb.transpose(1, 2)).squeeze(1) # [B, N]\n",
    "            return scores\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _create_causal_mask(self, seq_len, device):\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=device, dtype=torch.bool), diagonal=1)\n",
    "        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n",
    "        return mask\n",
    "\n",
    "    def predict_next(self, input_seq):\n",
    "        # Get sequence representations\n",
    "        seq_repr = self.forward(input_seq)  # [B, L, D]\n",
    "\n",
    "        # Use last position for prediction\n",
    "        last_hidden = seq_repr[:, -1, :]  # [B, D]\n",
    "\n",
    "        # Score against all item embeddings\n",
    "        all_item_embeds = self.item_embed.weight  # [num_items, D]\n",
    "        scores = torch.matmul(last_hidden, all_item_embeds.T)  # [B, num_items]\n",
    "        return scores"
   ],
   "id": "91f30ba4acb5fb2f",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:55:21.452535Z",
     "start_time": "2025-08-27T15:55:21.227253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load trained model on source domain\n",
    "def load_best_weights(model, ckpt_path=\"model/best_model.pth\", device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model = SASRec(\n",
    "    num_items=NUM_ITEMS_SOURCE,\n",
    "    hidden_dim=64,\n",
    "    max_seq_len=50,\n",
    "    num_blocks=2,\n",
    "    num_heads=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "best_model = load_best_weights(model, ckpt_path=\"model_sasrec/best_model.pth\", device=DEVICE)"
   ],
   "id": "96d2d9412fa45d5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SASRec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     10\u001B[39m     model.eval()\n\u001B[32m     11\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m model = \u001B[43mSASRec\u001B[49m(\n\u001B[32m     14\u001B[39m     num_items=NUM_ITEMS_SOURCE,\n\u001B[32m     15\u001B[39m     hidden_dim=\u001B[32m64\u001B[39m,\n\u001B[32m     16\u001B[39m     max_seq_len=\u001B[32m50\u001B[39m,\n\u001B[32m     17\u001B[39m     num_blocks=\u001B[32m2\u001B[39m,\n\u001B[32m     18\u001B[39m     num_heads=\u001B[32m2\u001B[39m,\n\u001B[32m     19\u001B[39m     dropout=\u001B[32m0.2\u001B[39m\n\u001B[32m     20\u001B[39m )\n\u001B[32m     22\u001B[39m best_model = load_best_weights(model, ckpt_path=\u001B[33m\"\u001B[39m\u001B[33mmodel_sasrec/best_model.pth\u001B[39m\u001B[33m\"\u001B[39m, device=DEVICE)\n",
      "\u001B[31mNameError\u001B[39m: name 'SASRec' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T15:35:39.394214Z",
     "start_time": "2025-08-27T15:35:39.389182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def compute_user_reprs_from_sequences(model_src, train_seqs_src, user_encoder_src, max_seq_len=50, device=DEVICE):\n",
    "    model_src.eval().to(device)\n",
    "    user_vecs = {}\n",
    "\n",
    "    for user_id, seq in train_seqs_src.items():\n",
    "        if len(seq) < 1:\n",
    "            continue\n",
    "\n",
    "        # Pad-left to max_seq_len\n",
    "        seq = seq[-max_seq_len:]\n",
    "        pad_len = max_seq_len - len(seq)\n",
    "        input_seq = torch.tensor([([0] * pad_len + seq)], dtype=torch.long, device=device)\n",
    "        hidden = model_src(input_seq)\n",
    "        last_hidden = hidden[0, -1, :].squeeze(0)\n",
    "        raw_user = user_encoder_src.inverse_transform([user_id])[0]\n",
    "        user_vecs[raw_user] = last_hidden.detach().cpu().numpy()\n",
    "\n",
    "    print(f\"Computed user representations for {len(user_vecs)} users.\")\n",
    "    return user_vecs\n",
    "\n",
    "user_vecs_src = compute_user_reprs_from_sequences(\n",
    "    model_src=\"model_sasrec/best_model.pth\",  # Path to the pre-trained source model\n",
    "\n",
    ")"
   ],
   "id": "22ce50e9b7b83585",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "96223fe47798e730"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cfeb162cd966672c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
