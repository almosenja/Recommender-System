{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Recommender System - Baseline Model",
   "id": "7c822c18c3c50055"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:57:06.445021Z",
     "start_time": "2025-08-23T11:57:06.438483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"D:/Python Projects/recommendation_system/recsys/data\"\n",
    "\n",
    "import kagglehub\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "id": "5a6345db3eba7a39",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:57:06.682028Z",
     "start_time": "2025-08-23T11:57:06.645915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)"
   ],
   "id": "edd5232c634edf29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Loading the dataset\n",
    "The dataset is Douban Cross Domain dataset from Kaggle."
   ],
   "id": "ff3e64dbef5ea775"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T12:17:48.139322Z",
     "start_time": "2025-08-23T12:17:41.957387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DOUBAN_DATASET = r\"fengzhujoey/douban-datasetratingreviewside-information\"\n",
    "DOUBAN_DOMAIN = \"moviereviews\"\n",
    "\n",
    "def load_douban_reviews(domain:str, max_items:int=100000) -> pd.DataFrame:\n",
    "    path = kagglehub.dataset_download(DOUBAN_DATASET)\n",
    "    df_raw = pd.read_csv(f\"{path}/douban_dataset(text information)/{DOUBAN_DOMAIN}_cleaned.txt\",\n",
    "                         sep=\"\\t\",\n",
    "                         quoting=csv.QUOTE_MINIMAL,\n",
    "                         dtype=str,\n",
    "                         keep_default_na=False)\n",
    "\n",
    "    df_raw.columns = [c.strip().strip('\"').strip('\"') for c in df_raw.columns]\n",
    "\n",
    "    # Coerce rating to float (invalid/missing -> 0.0)\n",
    "    ratings = pd.to_numeric(df_raw[\"rating\"], errors=\"coerce\").fillna(0.0).astype(float)\n",
    "\n",
    "    # Parse time to POSIX seconds (int)\n",
    "    timestamp = pd.to_datetime(df_raw[\"time\"], errors=\"coerce\", utc=True)\n",
    "    timestamp = timestamp.fillna(pd.Timestamp(\"1970-01-01\", tz=\"UTC\"))\n",
    "    timestamp_s = (timestamp.astype(int) // 10**9).astype(int)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"user\": df_raw[\"user_id\"].astype(int),\n",
    "        \"item\": df_raw[\"movie_id\"].astype(int),\n",
    "        \"rating\": ratings,\n",
    "        \"domain\": domain,\n",
    "        \"timestamp\": timestamp_s\n",
    "    })\n",
    "\n",
    "    if max_items:\n",
    "        out = out.copy()\n",
    "        out = out.sample(min(max_items, len(out))).reset_index(drop=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "df = load_douban_reviews(DOUBAN_DOMAIN, max_items=None)\n",
    "print(f\"Loaded {len(df)} rows from {DOUBAN_DOMAIN} domain.\")"
   ],
   "id": "44f0eeffce21c963",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1278401 rows from moviereviews domain.\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing the dataset\n",
    "- Make it implicit by considering all ratings as positive interactions.\n",
    "- Filter out users and items with less than 5 interactions.\n",
    "- Create a mapping of user and item IDs to indices."
   ],
   "id": "f1783303bdad3402"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:58:31.056411Z",
     "start_time": "2025-08-23T11:58:30.772893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Make implicit dataset, filter users/items with less than 5 interactions, and encode user/item IDs\n",
    "def preprocess_dataset(df, min_user_interactions=5, min_item_interactions=5):\n",
    "    # Make it implicit\n",
    "    df[\"label\"] = 1.0\n",
    "\n",
    "    # Filter users and items with less than 5 interactions\n",
    "    user_counts = df[\"user\"].value_counts()\n",
    "    item_counts = df[\"item\"].value_counts()\n",
    "\n",
    "    valid_users = user_counts[user_counts >= min_user_interactions].index\n",
    "    valid_items = item_counts[item_counts >= min_item_interactions].index\n",
    "\n",
    "    df = df[df[\"user\"].isin(valid_users) & df[\"item\"].isin(valid_items)].copy()\n",
    "    print(\"After interactions filtering:\", len(df), \"rows,\", df[\"user\"].nunique(), \"users,\", df[\"item\"].nunique(), \"items\")\n",
    "\n",
    "    user_enc = LabelEncoder()\n",
    "    item_enc = LabelEncoder()\n",
    "\n",
    "    df[\"user_id\"] = user_enc.fit_transform(df[\"user\"])\n",
    "    df[\"item_id\"] = item_enc.fit_transform(df[\"item\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "filtered_df = preprocess_dataset(df, min_user_interactions=20, min_item_interactions=20)"
   ],
   "id": "b9ebad4f47ee8759",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After interactions filtering: 1208553 rows, 2590 users, 14938 items\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T12:13:32.965655Z",
     "start_time": "2025-08-23T12:13:32.922156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_data_sparsity(df):\n",
    "    num_users = df[\"user\"].nunique()\n",
    "    num_items = df[\"item\"].nunique()\n",
    "    num_interactions = len(df)\n",
    "    density = num_interactions / (num_users * num_items)\n",
    "    sparsity = 1 - density\n",
    "\n",
    "    print(f\"Number of users: {num_users}\")\n",
    "    print(f\"Number of items: {num_items}\")\n",
    "    print(f\"Number of interactions: {num_interactions}\")\n",
    "    print(f\"-\" * 30)\n",
    "    print(f\"Interaction Matrix Density: {density:.4f}\")\n",
    "    print(f\"Interaction Matrix Sparsity: {sparsity:.4f}\")\n",
    "\n",
    "calculate_data_sparsity(filtered_df)"
   ],
   "id": "7d4bdc8be4449a7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users: 2590\n",
      "Number of items: 14938\n",
      "Number of interactions: 1208553\n",
      "------------------------------\n",
      "Interaction Matrix Density: 0.0312\n",
      "Interaction Matrix Sparsity: 0.9688\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:58:47.665952Z",
     "start_time": "2025-08-23T11:58:45.020509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define Leave-One-Out (LOO) split\n",
    "def loo_split(df):\n",
    "    df = df.sort_values([\"user_id\", \"timestamp\"])\n",
    "    train_rows, val_rows, test_rows = [], [], []\n",
    "    for uid, group in df.groupby(\"user_id\", sort=False):\n",
    "        g = group.sort_values(\"timestamp\")\n",
    "        if len(g) < 5:\n",
    "            train_rows.append(g)\n",
    "            continue\n",
    "        test_rows.append(g.iloc[[-1]])  # Last interaction as test\n",
    "        val_rows.append(g.iloc[[-2]])    # Second last interaction as validation\n",
    "        train_rows.append(g.iloc[:-2])    # All but last two as training\n",
    "\n",
    "    train_df = pd.concat(train_rows, ignore_index=True)\n",
    "    val_df = pd.concat(val_rows, ignore_index=True)\n",
    "    test_df = pd.concat(test_rows, ignore_index=True)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "train_df, val_df, test_df = loo_split(filtered_df)\n",
    "print(f\"Train/Validation/Test split: {len(train_df)}, {len(val_df)}, {len(test_df)}\")"
   ],
   "id": "1bcb3147c4b71b5e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Validation/Test split: 1203373, 2590, 2590\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Building the dataset with negative sampling",
   "id": "4f6e8d4cae86c716"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:58:49.696965Z",
     "start_time": "2025-08-23T11:58:49.690433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "NUM_USERS = filtered_df[\"user_id\"].max() + 1\n",
    "NUM_ITEMS = filtered_df[\"item_id\"].max() + 1"
   ],
   "id": "265b459b4730043d",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:58:52.433583Z",
     "start_time": "2025-08-23T11:58:50.668924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rank the single positive against k negative samples for evaluation\n",
    "pos_items_by_user = defaultdict(set)\n",
    "for u, i in zip(train_df[\"user_id\"].values, train_df[\"item_id\"].values):\n",
    "    pos_items_by_user[u].add(i)\n",
    "\n",
    "def sample_eval_negatives(eval_df, num_items, pos_by_user, n_neg=99, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    neg_dict = {}\n",
    "    for u, pos_i in zip(eval_df[\"user_id\"].values, eval_df[\"item_id\"].values):\n",
    "        user_pos = pos_by_user[u].copy()\n",
    "        user_pos.add(pos_i)\n",
    "        candidates = []\n",
    "        while len(candidates) < n_neg:\n",
    "            cand = rng.integers(0, num_items)\n",
    "            if cand not in user_pos:\n",
    "                candidates.append(int(cand))\n",
    "                user_pos.add(cand)\n",
    "        neg_dict[u] = candidates\n",
    "    return neg_dict\n",
    "\n",
    "val_negatives = sample_eval_negatives(val_df, NUM_ITEMS, pos_items_by_user)\n",
    "test_negatives = sample_eval_negatives(test_df, NUM_ITEMS, pos_items_by_user)\n",
    "print(f\"Sampled {len(val_negatives)} validation users and {len(test_negatives)} test users with {len(next(iter(val_negatives.values())))} negatives each.\")"
   ],
   "id": "cca53221665f9295",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 2590 validation users and 2590 test users with 99 negatives each.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:58:55.758749Z",
     "start_time": "2025-08-23T11:58:55.737574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training dataset with k negatives per positive\n",
    "class ImplicitTrainingDataset(Dataset):\n",
    "    def __init__(self, train_df, num_items, pos_by_user, neg_k=4):\n",
    "        self.pos_pairs = train_df[[\"user_id\", \"item_id\"]].values.astype(np.int64)\n",
    "        self.num_pos = len(self.pos_pairs)\n",
    "        self.neg_k = neg_k\n",
    "        self.num_items = num_items\n",
    "        self.pos_by_user = pos_by_user\n",
    "        self.length = self.num_pos * (1 + self.neg_k)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pos_idx = idx // (self.neg_k + 1)\n",
    "        is_pos = (idx % (self.neg_k + 1) == 0)\n",
    "        u, i_pos = self.pos_pairs[pos_idx]\n",
    "        if is_pos:\n",
    "            return int(u), int(i_pos), 1.0\n",
    "\n",
    "        while True:\n",
    "            j = random.randint(0, self.num_items - 1)\n",
    "            if j not in self.pos_by_user[u]:\n",
    "                return int(u), int(j), 0.0\n",
    "\n",
    "train_dataset = ImplicitTrainingDataset(train_df,\n",
    "                                  num_items=NUM_ITEMS,\n",
    "                                  pos_by_user=pos_items_by_user,\n",
    "                                  neg_k=4)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=1024,\n",
    "                          shuffle=True)\n",
    "\n",
    "print(f\"Training dataset created with {len(train_dataset)} samples.\")"
   ],
   "id": "ca3c4cf5ec1ad7b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset created with 6016865 samples.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Utility functions for evaluation & ranking metrics\n",
    "- For each val user, build candidate set = `{positive} U {k negatives}`\n",
    "- Score, sort, compute HR@K, NDCG@K, Precision@K, Recall@K"
   ],
   "id": "5091158ecaf52022"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:58:59.054567Z",
     "start_time": "2025-08-23T11:58:59.049732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @torch.no_grad()\n",
    "# def compute_val_loss(model, eval_df, loss_fn, pos_items_by_user, num_items, neg_k=4, device=\"cpu\"):\n",
    "#     model.eval()\n",
    "#     losses = []\n",
    "#     for u, i_pos in zip(eval_df[\"user_id\"].values, eval_df[\"item_d\"].values):\n",
    "#         # Positives\n",
    "#         u_t = torch.tensor([u], dtype=torch.long, device=device)\n",
    "#         i_t = torch.tensor([i_pos], dtype=torch.long, device=device)\n",
    "#         logit_pos = model(u_t, i_t)\n",
    "#         loss_pos = loss_fn(logit_pos, torch.ones_like(logit_pos))\n",
    "#         losses.append(loss_pos.item())\n",
    "#\n",
    "#         # Negatives\n",
    "#         taken = pos_items_by_user[u].copy()\n",
    "#         taken.add(int(i_pos))\n",
    "#         n_added = 0\n",
    "#         while n_added < neg_k:\n",
    "#             j = random.randint(0, num_items - 1)\n",
    "#             if j not in taken:\n",
    "#                 uj = torch.tensor([u], dtype=torch.long, device=device)\n",
    "#                 jj = torch.tensor([j], dtype=torch.long, device=device)\n",
    "#                 logit_neg = model(uj, jj)\n",
    "#                 loss_neg = loss_fn(logit_neg, torch.zeros_like(logit_neg))\n",
    "#                 losses.append(loss_neg.item())\n",
    "#                 taken.add(j)\n",
    "#                 n_added += 1\n",
    "#\n",
    "#     return float(np.mean(losses))\n",
    "#\n",
    "# def evaluate_ranking(eval_df, n_neg, seed=42):\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     neg_dict = {}\n",
    "#"
   ],
   "id": "985bafb819834daf",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:59:00.046999Z",
     "start_time": "2025-08-23T11:59:00.037515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Validation loss and ranking metrics\n",
    "@torch.no_grad()\n",
    "def evaluate_ranking(model, eval_df, neg_dict, k=10, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    hits, ndcgs, precisions, recalls = [], [], [], []\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    for u, pos_i in zip(eval_df[\"user_id\"].values, eval_df[\"item_id\"].values):\n",
    "        candidates = [int(pos_i)] + neg_dict[u]\n",
    "        users = torch.tensor([u] * len(candidates), dtype=torch.long, device=device)\n",
    "        items = torch.tensor(candidates, dtype=torch.long, device=device)\n",
    "\n",
    "        scores = model(users, items).detach().cpu().numpy()\n",
    "\n",
    "        # Tiny noise to break ties fairly (same user → same noise stream)\n",
    "        user_rng = np.random.default_rng(rng.integers(0, 2**31) ^ u)\n",
    "        scores = scores + 1e-12 * user_rng.standard_normal(scores.shape)\n",
    "        rank = np.argsort(-scores).tolist().index(0) + 1\n",
    "        # rank = 1 + int(np.sum(scores[1:] > scores[0]))\n",
    "\n",
    "        hit = 1.0 if rank <= k else 0.0\n",
    "        hits.append(hit)\n",
    "        ndcg = (1.0 / math.log2(rank + 1)) if rank <= k else 0.0\n",
    "        ndcgs.append(ndcg)\n",
    "        precisions.append(hit / k)\n",
    "        recalls.append(hit)\n",
    "\n",
    "    return {\n",
    "        \"HR@K\": float(np.mean(hits)),\n",
    "        \"NDCG@K\": float(np.mean(ndcgs)),\n",
    "        \"Precision@K\": float(np.mean(precisions)),\n",
    "        \"Recall@K\": float(np.mean(recalls))\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_val_loss(model, eval_df, loss_fn, pos_items_by_user, num_items, neg_k=4, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for u, i_pos in zip(eval_df[\"user_id\"].values, eval_df[\"item_id\"].values):\n",
    "        # Positives\n",
    "        u_t = torch.tensor([u], dtype=torch.long, device=device)\n",
    "        i_t = torch.tensor([i_pos], dtype=torch.long, device=device)\n",
    "        logit_pos = model(u_t, i_t)\n",
    "        loss_pos = loss_fn(logit_pos, torch.ones_like(logit_pos))\n",
    "        losses.append(loss_pos.item())\n",
    "\n",
    "        # Negatives\n",
    "        taken = pos_items_by_user[u].copy()\n",
    "        taken.add(int(i_pos))\n",
    "        n_added = 0\n",
    "        while n_added < neg_k:\n",
    "            j = random.randint(0, num_items - 1)\n",
    "            if j not in taken:\n",
    "                jj = torch.tensor([j], dtype=torch.long, device=device)\n",
    "                logit_neg = model(u_t, jj)\n",
    "                loss_neg = loss_fn(logit_neg, torch.zeros_like(logit_neg))\n",
    "                losses.append(loss_neg.item())\n",
    "                taken.add(j)\n",
    "                n_added += 1\n",
    "\n",
    "    return float(np.mean(losses))"
   ],
   "id": "778028709a077112",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Building baseline model",
   "id": "813ae516d9ed3c96"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:59:01.698720Z",
     "start_time": "2025-08-23T11:59:01.694497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Simple Matrix Factorization model with biases (dot product)\n",
    "# class MF(nn.Module):\n",
    "#     def __init__(self, num_users, num_items, embed_dim=64):\n",
    "#         super().__init__()\n",
    "#         self.user_emb = nn.Embedding(num_users, embed_dim)\n",
    "#         self.item_emb = nn.Embedding(num_items, embed_dim)\n",
    "#         nn.init.normal_(self.user_emb.weight, std=0.01)\n",
    "#         nn.init.normal_(self.item_emb.weight, std=0.01)\n",
    "#\n",
    "#     def forward(self, users, items):\n",
    "#         u = self.user_emb(users)\n",
    "#         v = self.item_emb(items)\n",
    "#         output = (u * v).sum(dim=1, keepdim=True)\n",
    "#         return output.squeeze()"
   ],
   "id": "6b90e93ceaa304db",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:59:02.193050Z",
     "start_time": "2025-08-23T11:59:02.186084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ImprovedMatrixFactorization(nn.Module):\n",
    "    \"\"\"Improved MF model with dropout and better initialization\"\"\"\n",
    "    def __init__(self, n_users, n_items, embedding_dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Better initialization\n",
    "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_embeds = self.dropout(self.user_embedding(user_ids))\n",
    "        item_embeds = self.dropout(self.item_embedding(item_ids))\n",
    "\n",
    "        dot_product = (user_embeds * item_embeds).sum(dim=1, keepdim=True)\n",
    "        output = dot_product + self.user_bias(user_ids) + self.item_bias(item_ids) + self.global_bias\n",
    "\n",
    "        return output.squeeze()\n",
    "\n",
    "    def predict_all_items(self, user_id):\n",
    "        \"\"\"Predict scores for all items for a given user\"\"\"\n",
    "        user_tensor = torch.tensor([user_id], dtype=torch.long).to(next(self.parameters()).device)\n",
    "        all_items = torch.arange(self.item_embedding.num_embeddings).to(next(self.parameters()).device)\n",
    "\n",
    "        user_embed = self.user_embedding(user_tensor)\n",
    "        item_embeds = self.item_embedding(all_items)\n",
    "\n",
    "        scores = torch.matmul(user_embed, item_embeds.T).squeeze()\n",
    "        scores += self.user_bias(user_tensor).squeeze()\n",
    "        scores += self.item_bias(all_items).squeeze()\n",
    "        scores += self.global_bias\n",
    "\n",
    "        return torch.sigmoid(scores)"
   ],
   "id": "caa02604866880b8",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train loop",
   "id": "1cd309ea5d673cac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T12:01:57.137744Z",
     "start_time": "2025-08-23T11:59:03.732448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 30\n",
    "K = 10 # Cutoff for ranking metrics\n",
    "\n",
    "model = ImprovedMatrixFactorization(n_users=NUM_USERS, n_items=NUM_ITEMS, embedding_dim=64).to(DEVICE)\n",
    "# model = MF(num_users=NUM_USERS, num_items=NUM_ITEMS, embed_dim=32).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "best_ndcg = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for users, items, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\"):\n",
    "        users = users.to(DEVICE)\n",
    "        items = items.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(users, items)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    train_loss = running_loss / max(n_batches, 1)\n",
    "    val_loss = compute_val_loss(model, val_df, loss_fn, pos_items_by_user, NUM_ITEMS, neg_k=4, device=DEVICE)\n",
    "    metrics = evaluate_ranking(model, val_df, val_negatives, k=K, device=DEVICE)\n",
    "\n",
    "    improvement_msg = \"\"\n",
    "    cur_ndcg = metrics[\"NDCG@K\"]\n",
    "    if cur_ndcg > best_ndcg:\n",
    "        improvement_msg = f\"   <--- New best NDCG@{K}: {cur_ndcg:.4f}\"\n",
    "        best_ndcg = cur_ndcg\n",
    "        best_epoch = epoch + 1\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, \"\n",
    "          f\"HR@{K}: {metrics['HR@K']:.4f}, \"\n",
    "          f\"NDCG@{K}: {metrics['NDCG@K']:.4f}, \"\n",
    "          f\"Precision@{K}: {metrics['Precision@K']:.4f}, \"\n",
    "          f\"Recall@{K}: {metrics['Recall@K']:.4f}, \"\n",
    "          f\"{improvement_msg}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best performance was at Epoch {best_epoch} with NDCG@{K}: {best_ndcg:.4f}\")\n",
    "print(\"=\"*50)"
   ],
   "id": "87d08c742d020d5f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30: 100%|██████████| 5876/5876 [00:45<00:00, 128.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Train Loss: 0.5165, Val Loss: 0.4991, HR@10: 0.5429, NDCG@10: 0.3398, Precision@10: 0.0543, Recall@10: 0.5429,    <--- New best NDCG@10: 0.3398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30: 100%|██████████| 5876/5876 [00:44<00:00, 131.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 - Train Loss: 0.4994, Val Loss: 0.4992, HR@10: 0.5629, NDCG@10: 0.3227, Precision@10: 0.0563, Recall@10: 0.5629, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30: 100%|██████████| 5876/5876 [00:44<00:00, 131.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 - Train Loss: 0.4994, Val Loss: 0.4991, HR@10: 0.5556, NDCG@10: 0.3405, Precision@10: 0.0556, Recall@10: 0.5556,    <--- New best NDCG@10: 0.3405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30:  17%|█▋        | 975/5876 [00:07<00:37, 130.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[43]\u001B[39m\u001B[32m, line 18\u001B[39m\n\u001B[32m     15\u001B[39m n_batches = \u001B[32m0\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m users, items, labels \u001B[38;5;129;01min\u001B[39;00m tqdm(train_loader, desc=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mEpoch \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m+\u001B[38;5;250m \u001B[39m\u001B[32m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mEPOCHS\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m):\n\u001B[32m---> \u001B[39m\u001B[32m18\u001B[39m     users = \u001B[43musers\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDEVICE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m     items = items.to(DEVICE)\n\u001B[32m     20\u001B[39m     labels = labels.to(DEVICE)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:15:50.109817Z",
     "start_time": "2025-08-23T11:15:50.106830Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "62c8b8450eda9fb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T11:15:50.123946Z",
     "start_time": "2025-08-23T11:15:50.121840Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "fd43c719445c714e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
