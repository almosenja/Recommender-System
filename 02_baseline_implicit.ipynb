{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-21T15:27:02.596315Z",
     "start_time": "2025-08-21T15:26:55.829041Z"
    }
   },
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from recommenders.datasets.python_splitters import python_chrono_split, python_stratified_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# os.environ[\"HF_HOME\"] = \"E:/Python Scripts/recsys\"\n",
    "# os.environ['HF_DATASETS_CACHE'] = \"E:/Python Scripts/recsys/data\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = \"E:/Python Scripts/recsys/models\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = \"D:/Python Projects/recommendation_system\"\n",
    "os.environ['HF_DATASETS_CACHE'] = \"D:/Python Projects/recommendation_system/recsys/data\"\n",
    "os.environ['TRANSFORMERS_CACHE'] = \"D:/Python Projects/recommendation_system/recsys/models\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from tensorboardX import SummaryWriter"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:27:02.686145Z",
     "start_time": "2025-08-21T15:27:02.605410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ],
   "id": "6217117094a61052",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:27:27.746834Z",
     "start_time": "2025-08-21T15:27:02.691671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Select 2 categories to highlight cross-domain transfer\n",
    "SOURCE_DOMAIN = \"Movies_and_TV\"\n",
    "TARGET_DOMAIN = \"Video_Games\"\n",
    "DOMAINS = [SOURCE_DOMAIN, TARGET_DOMAIN]\n",
    "\n",
    "MIN_USER_INTERACTIONS = 10\n",
    "MIN_ITEM_INTERACTIONS = 10\n",
    "POSITIVE_THRESHOLD = 4.0  # Ratings >= 4.0 are considered positive\n",
    "\n",
    "# Load the dataset\n",
    "def load_amazon_reviews(domain:str, max_per_domain:int=100000) -> pd.DataFrame:\n",
    "    dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "                           f\"raw_review_{domain}\",\n",
    "                           trust_remote_code=True)\n",
    "    rows = []\n",
    "    for i, r in enumerate(dataset[\"full\"]):\n",
    "        if i >= max_per_domain:\n",
    "            break\n",
    "        rows.append({\n",
    "            \"user\": r[\"user_id\"],\n",
    "            \"item\": r[\"parent_asin\"],\n",
    "            \"rating\": float(r[\"rating\"]),\n",
    "            \"domain\": domain,\n",
    "            \"verified_purchase\": r[\"verified_purchase\"],\n",
    "            \"timestamp\": int(r[\"timestamp\"])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Sample 100k reviews per domain for model development\n",
    "# dfs = [load_amazon_reviews(dom, max_per_domain=300000) for dom in DOMAINS]\n",
    "# df = pd.concat(dfs, ignore_index=True).sort_values(\"timestamp\").reset_index(drop=True)\n",
    "df = load_amazon_reviews(SOURCE_DOMAIN, max_per_domain=100000)"
   ],
   "id": "1e3547522e86afe6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:27:27.757407Z",
     "start_time": "2025-08-21T15:27:27.752911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RatingDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, user_list, item_list, rating_list):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.user_list = user_list\n",
    "\t\tself.item_list = item_list\n",
    "\t\tself.rating_list = rating_list\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.user_list)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tuser = self.user_list[idx]\n",
    "\t\titem = self.item_list[idx]\n",
    "\t\trating = self.rating_list[idx]\n",
    "\n",
    "\t\treturn (\n",
    "\t\t\ttorch.tensor(user, dtype=torch.long),\n",
    "\t\t\ttorch.tensor(item, dtype=torch.long),\n",
    "\t\t\ttorch.tensor(rating, dtype=torch.float)\n",
    "\t\t\t)"
   ],
   "id": "539a5c41dd5954b8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:52:39.864069Z",
     "start_time": "2025-08-21T15:52:39.838111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NCFData(object):\n",
    "    def __init__(self, df, num_neg=4, num_neg_test=99, batch_size=1024):\n",
    "        self.df = df\n",
    "        self.num_neg = num_neg\n",
    "        self.num_neg_test = num_neg_test\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.preprocess_df = self._reindex(self.df)\n",
    "\n",
    "        ok_users = self.preprocess_df.groupby(\"user_id\").size()\n",
    "        ok_users = ok_users[ok_users >= 2].index\n",
    "        self.preprocess_df = self.preprocess_df[self.preprocess_df.user_id.isin(ok_users)].copy()\n",
    "\n",
    "        self.user_pool = set(self.preprocess_df[\"user_id\"].unique().tolist())\n",
    "        self.item_pool = set(self.preprocess_df[\"item_id\"].unique().tolist())\n",
    "\n",
    "        self.train_df, self.val_df, self.test_df = self._leave_one_out(self.preprocess_df)\n",
    "        self.negatives = self._negative_sampling(self.preprocess_df)\n",
    "\n",
    "    def _reindex(self, df):\n",
    "        \"\"\"Preprocess dataset to reindex userID and itemID, also set rating as binary feedback\"\"\"\n",
    "        print(\"Reindexing users and items...\")\n",
    "        df = df.copy()\n",
    "        u2id = {u: idx for idx, u in enumerate(df[\"user\"].dropna().unique())}\n",
    "        i2id = {i: idx for idx, i in enumerate(df[\"item\"].dropna().unique())}\n",
    "        df = df.dropna(subset=[\"user\", \"item\", \"rating\", \"timestamp\"]).copy()\n",
    "\n",
    "        df[\"user_id\"] = df[\"user\"].apply(lambda x: u2id[x]).astype(np.int64)\n",
    "        df[\"item_id\"] = df[\"item\"].apply(lambda x: i2id[x]).astype(np.int64)\n",
    "        df[\"rating\"] = df[\"rating\"].apply(lambda x: x > 0).astype(np.float32)\n",
    "        print(f\"Reindexed {len(u2id)} users and {len(i2id)} items.\\n\")\n",
    "        return df\n",
    "\n",
    "    def _leave_one_out(self, df):\n",
    "        \"\"\"Leave one out split for training and testing\"\"\"\n",
    "        print(\"Leave one out split...\")\n",
    "        df = df.sort_values([\"user_id\", \"timestamp\"], ascending=[True, True]).copy()\n",
    "        df[\"rank_latest\"] = df.groupby(\"user_id\")[\"timestamp\"].rank(method=\"first\", ascending=False)\n",
    "\n",
    "        train = df.loc[df[\"rank_latest\"] > 2, [\"user_id\", \"item_id\", \"rating\"]].copy()\n",
    "        val = df.loc[df[\"rank_latest\"] == 1, [\"user_id\", \"item_id\", \"rating\"]].copy()\n",
    "        test = df.loc[df[\"rank_latest\"] == 2, [\"user_id\", \"item_id\", \"rating\"]].copy()\n",
    "\n",
    "        # Ensure same user sets exist in train/test\n",
    "        common_users = set(train[\"user_id\"].unique()) & set(test[\"user_id\"].unique())\n",
    "        train = train[train[\"user_id\"].isin(common_users)]\n",
    "        val = val[val[\"user_id\"].isin(common_users)]\n",
    "        test = test[test[\"user_id\"].isin(common_users)]\n",
    "        print(f\"Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\\n\")\n",
    "        return train, val, test\n",
    "\n",
    "    def _negative_sampling(self, df):\n",
    "        \"\"\"Negative sampling for training\"\"\"\n",
    "        print(\"Negative sampling...\")\n",
    "        interact_status = (\n",
    "            df.groupby(\"user_id\")[\"item_id\"]\n",
    "            .apply(set)\n",
    "            .reset_index()\n",
    "            .rename(columns={\"item_id\": \"interacted_items\"}))\n",
    "        all_items = self.item_pool\n",
    "\n",
    "        def sample_negatives(user_set, k):\n",
    "            pool = list(all_items - user_set)\n",
    "            if len(pool) == 0:\n",
    "                return []\n",
    "            k = min(k, len(pool))\n",
    "            return random.sample(pool, k)\n",
    "\n",
    "        interact_status[\"negative_items\"] = (interact_status[\"interacted_items\"].apply(\n",
    "            lambda x: all_items - x))\n",
    "        interact_status[\"negative_samples\"] = (interact_status[\"negative_items\"].apply(\n",
    "            lambda x: sample_negatives(x, self.num_neg_test)))\n",
    "        print(f\"Negative sampling done. Sampled {self.num_neg_test} negatives per user.\\n\")\n",
    "        return interact_status[[\"user_id\", \"interacted_items\", \"negative_items\", \"negative_samples\"]]\n",
    "\n",
    "    def get_train_instance(self):\n",
    "        \"\"\"Get training instances with negative sampling\"\"\"\n",
    "        print(\"Getting training instances...\")\n",
    "        users, items, ratings = [], [], []\n",
    "        train_df = pd.merge(self.train_df,\n",
    "                            self.negatives[[\"user_id\", \"negative_items\"]],\n",
    "                            on=\"user_id\",\n",
    "                            how=\"inner\")\n",
    "\n",
    "        train_df[\"negatives\"] = (train_df[\"negative_items\"].apply(\n",
    "            lambda x: random.sample(list(x), min(self.num_neg, len(x))) if len(x) > 0 else []))\n",
    "\n",
    "        for row in train_df.itertuples(index=False):\n",
    "            users.append(int(row[\"user_id\"]))\n",
    "            items.append(int(row[\"item_id\"]))\n",
    "            ratings.append(float(row[\"rating\"]))\n",
    "            for neg in row[\"negatives\"]:\n",
    "                users.append(int(row[\"user_id\"]))\n",
    "                items.append(int(row[\"negatives\"]))\n",
    "                ratings.append(float(0))  # negative samples get 0 rating\n",
    "\n",
    "        dataset = RatingDataset(users, items, ratings)\n",
    "        print(f\"Total training instances: {len(users)}\\n\")\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "    def _pack_eval(self, split_df):\n",
    "        \"\"\"Pack evaluation instances with negative sampling\"\"\"\n",
    "        users, items, ratings = [], [], []\n",
    "        eval_df = pd.merge(split_df,\n",
    "                           self.negatives[[\"user_id\", \"negative_samples\"]],\n",
    "                           on=\"user_id\",\n",
    "                           how=\"inner\")\n",
    "\n",
    "        for row in eval_df.itertuples(index=False):\n",
    "            # positive first, then its negatives (candidate set == 1 + num_neg_test)\n",
    "            users.append(int(row.user_id))\n",
    "            items.append(int(row.item_id))\n",
    "            ratings.append(float(row.rating))\n",
    "            for neg in row[\"negative_samples\"]:\n",
    "                users.append(int(row[\"user_id\"]))\n",
    "                items.append(int(neg))\n",
    "                ratings.append(float(0))  # negative samples get 0 rating\n",
    "\n",
    "        dataset = RatingDataset(users, items, ratings)\n",
    "        print(f\"Total {split_df.name} instances: {len(users)}\\n\")\n",
    "        return DataLoader(dataset, batch_size=self.num_neg_test + 1, shuffle=False, drop_last=False)\n",
    "\n",
    "    def get_val_instance(self):\n",
    "        \"\"\"Get val instances with negative sampling\"\"\"\n",
    "        print(\"Getting validation instances...\")\n",
    "        return self._pack_eval(self.val_df)\n",
    "\n",
    "    def get_test_instance(self):\n",
    "        \"\"\"Get test instances with negative sampling\"\"\"\n",
    "        print(\"Getting test instances...\")\n",
    "        return self._pack_eval(self.test_df)"
   ],
   "id": "e97a42666a7429d3",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:52:40.156016Z",
     "start_time": "2025-08-21T15:52:40.148650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hit(ng_item, pred_items):\n",
    "    return 1 if ng_item in pred_items else 0\n",
    "\n",
    "def ndcg(ng_item, pred_items):\n",
    "    if ng_item in pred_items:\n",
    "        idx = pred_items.index(ng_item)\n",
    "        return float(1.0 / np.log2(idx + 2))\n",
    "    return 0.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def metrics(model, test_loader, top_k, device):\n",
    "    model.eval()\n",
    "    hr, ndcg = [], []\n",
    "    for user, item, label in test_loader:\n",
    "        user = user.to(device); item = item.to(device)\n",
    "        preds = model(user, item)\n",
    "        _, indices = torch.topk(preds, k=top_k)\n",
    "        recommends = torch.take(item, indices).cpu().tolist()\n",
    "        ng_item = int(item[0].item())   # first element is the positive by construction\n",
    "        hr.append(hit(ng_item, recommends))\n",
    "        ndcg.append(ndcg(ng_item, recommends))\n",
    "    return float(np.mean(hr)), float(np.mean(ndcg))"
   ],
   "id": "4dd78b23545e1899",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:54:58.104358Z",
     "start_time": "2025-08-21T15:54:58.054634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleMatrixFactorization(nn.Module):\n",
    "    def __init__(self, n_users, n_items, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.user_embedding = nn.Embedding(n_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(n_items, embedding_dim)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.item_bias = nn.Embedding(n_items, 1)\n",
    "        self.global_bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        nn.init.normal_(self.user_embedding.weight, std=0.05)\n",
    "        nn.init.normal_(self.item_embedding.weight, std=0.05)\n",
    "        nn.init.zeros_(self.user_bias.weight)\n",
    "        nn.init.zeros_(self.item_bias.weight)\n",
    "\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        ue = self.user_embedding(user_ids)\n",
    "        ie = self.item_embedding(item_ids)\n",
    "        dot = (ue * ie).sum(dim=1, keepdim=True)\n",
    "        out = dot + self.user_bias(user_ids) + self.item_bias(item_ids) + self.global_bias\n",
    "        return out.squeeze(1)"
   ],
   "id": "ef00799043f0628b",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:52:41.329239Z",
     "start_time": "2025-08-21T15:52:41.323360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# class GeneralizedMatrixFactorization(nn.Module):\n",
    "#     def __init__(self, num_users, num_items, num_embeddings=32):\n",
    "#         super().__init__()\n",
    "#         self.user_embeddings = nn.Embedding(num_users, num_embeddings)\n",
    "#         self.item_embeddings = nn.Embedding(num_items, num_embeddings)\n",
    "#         self.affine_output = nn.Linear(in_features=num_embeddings, out_features=1)\n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "#\n",
    "#     def forward(self, user_ids, item_ids):\n",
    "#         user_embedding = self.user_embeddings(user_ids)\n",
    "#         item_embedding = self.item_embeddings(item_ids)\n",
    "#         element_product = torch.mul(user_embedding, item_embedding)\n",
    "#         logits = self.affine_output(element_product)\n",
    "#         rating = self.sigmoid(logits)\n",
    "#         return rating\n",
    "#\n",
    "#     def init_weight(self):\n",
    "#         pass\n",
    "#\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, num_users, num_items, num_embeddings=32, num_layers=3):\n",
    "#         super().__init__()\n",
    "#         self.user_embeddings = nn.Embedding(num_users, num_embeddings)\n",
    "#         self.item_embeddings = nn.Embedding(num_items, num_embeddings)\n",
    "#\n",
    "#         self.fc_layers = nn.ModuleList()\n",
    "#         for idx, (in_size, out_size) in enumerate(zip(num_layers[:-1], num_layers[1:])):\n",
    "#             self.fc_layers.append(nn.Linear(in_size, out_size))\n",
    "#\n",
    "#         self.affine_output = nn.Linear(num_layers[-1], out_features=1)\n",
    "#         self.logistic = nn.Sigmoid()\n",
    "#\n",
    "#     def forward(self, user_indices, item_indices):\n",
    "#         user_embedding = self.embedding_user(user_indices)\n",
    "#         item_embedding = self.embedding_item(item_indices)\n",
    "#         vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
    "#         for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "#             vector = self.fc_layers[idx](vector)\n",
    "#             vector = nn.ReLU()(vector)\n",
    "#             # vector = nn.BatchNorm1d()(vector)\n",
    "#             # vector = nn.Dropout(p=0.5)(vector)\n",
    "#         logits = self.affine_output(vector)\n",
    "#         rating = self.logistic(logits)\n",
    "#         return rating\n",
    "#\n",
    "#     def init_weight(self):\n",
    "#         pass\n",
    "#\n",
    "# class NeuMF(nn.Module):\n",
    "#     def __init__(self, args, num_users, num_items):\n",
    "#         super(NeuMF, self).__init__()\n",
    "#         self.num_users = num_users\n",
    "#         self.num_items = num_items\n",
    "#         self.factor_num_mf = args.factor_num\n",
    "#         self.factor_num_mlp =  int(args.layers[0]/2)\n",
    "#         self.layers = args.layers\n",
    "#         self.dropout = args.dropout\n",
    "#\n",
    "#         self.embedding_user_mlp = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mlp)\n",
    "#         self.embedding_item_mlp = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mlp)\n",
    "#\n",
    "#         self.embedding_user_mf = nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.factor_num_mf)\n",
    "#         self.embedding_item_mf = nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.factor_num_mf)\n",
    "#\n",
    "#         self.fc_layers = nn.ModuleList()\n",
    "#         for idx, (in_size, out_size) in enumerate(zip(args.layers[:-1], args.layers[1:])):\n",
    "#             self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
    "#             self.fc_layers.append(nn.ReLU())\n",
    "#\n",
    "#         self.affine_output = nn.Linear(in_features=args.layers[-1] + self.factor_num_mf, out_features=1)\n",
    "#         self.logistic = nn.Sigmoid()\n",
    "#         self.init_weight()\n",
    "#\n",
    "#     def init_weight(self):\n",
    "#         nn.init.normal_(self.embedding_user_mlp.weight, std=0.01)\n",
    "#         nn.init.normal_(self.embedding_item_mlp.weight, std=0.01)\n",
    "#         nn.init.normal_(self.embedding_user_mf.weight, std=0.01)\n",
    "#         nn.init.normal_(self.embedding_item_mf.weight, std=0.01)\n",
    "#\n",
    "#         for m in self.fc_layers:\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.xavier_uniform_(m.weight)\n",
    "#\n",
    "#         nn.init.xavier_uniform_(self.affine_output.weight)\n",
    "#\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "#                 m.bias.data.zero_()\n",
    "#\n",
    "#     def forward(self, user_indices, item_indices):\n",
    "#         user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
    "#         item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
    "#\n",
    "#         user_embedding_mf = self.embedding_user_mf(user_indices)\n",
    "#         item_embedding_mf = self.embedding_item_mf(item_indices)\n",
    "#\n",
    "#         mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)\n",
    "#         mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
    "#\n",
    "#         for idx, _ in enumerate(range(len(self.fc_layers))):\n",
    "#             mlp_vector = self.fc_layers[idx](mlp_vector)\n",
    "#\n",
    "#         vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
    "#         logits = self.affine_output(vector)\n",
    "#         rating = self.logistic(logits)\n",
    "#         return rating.squeeze()"
   ],
   "id": "bbf4698e0323fa4b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T15:59:02.899351800Z",
     "start_time": "2025-08-21T15:55:04.410590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = NCFData(df, num_neg=4, num_neg_test=99, batch_size=1024)\n",
    "train_loader = data.get_train_instance()\n",
    "val_loader   = data.get_val_instance()\n",
    "test_loader  = data.get_test_instance()\n",
    "\n",
    "n_users = int(data.preprocess_df[\"user_id\"].nunique())\n",
    "n_items = int(data.preprocess_df[\"item_id\"].nunique())\n",
    "embedding_dim = 32\n",
    "epochs = 10\n",
    "\n",
    "model = SimpleMatrixFactorization(n_users, n_items, embedding_dim).to(DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # <-- changed\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "global_step = 0\n",
    "best_hr = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "\n",
    "    for user, item, label in train_loader:\n",
    "        user = user.to(DEVICE); item = item.to(DEVICE); label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(user, item)\n",
    "        loss = loss_fn(logits, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"loss/train\", float(loss.item()), global_step)\n",
    "        global_step += 1\n",
    "\n",
    "    hr, ndcg = metrics(model, test_loader, top_k=10, device=DEVICE)\n",
    "    writer.add_scalar(\"perf/HR@10\", hr, epoch)\n",
    "    writer.add_scalar(\"perf/NDCG@10\", ndcg, epoch)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"The time elapsed for epoch {epoch+1} is {time.strftime('%H:%M:%S', time.gmtime(elapsed_time))} seconds\")\n",
    "    print(f\"Epoch {epoch+1}: HR@10 = {hr:.4f}, NDCG@10 = {ndcg:.4f}\")\n",
    "\n",
    "writer.close()"
   ],
   "id": "840a75ddb8889143",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reindexing users and items...\n",
      "Reindexed 17217 users and 56308 items.\n",
      "\n",
      "Leave one out split...\n",
      "Train: 72399, Val: 7347, Test: 7347\n",
      "\n",
      "Negative sampling...\n",
      "Negative sampling done. Sampled 99 negatives per user.\n",
      "\n",
      "Getting training instances...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6bac60307ff2fea6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
